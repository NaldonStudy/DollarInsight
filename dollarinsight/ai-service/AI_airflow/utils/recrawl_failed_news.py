# -*- coding: utf-8 -*-

"""
ë´‡ ê°ì§€ë¡œ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ë‰´ìŠ¤ë¥¼ URLë¡œ ë‹¤ì‹œ í¬ë¡¤ë§í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ë¡œì»¬ ì‹¤í–‰ìš©)
"""

import json
import sys
import time
from pathlib import Path
from typing import List, Dict
from datetime import datetime
from playwright.sync_api import sync_playwright


def is_bot_detected(article: Dict) -> bool:
    """ë´‡ ê°ì§€ë¡œ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ í•­ëª©ì¸ì§€ í™•ì¸"""
    title = article.get("title", "").strip()
    content = article.get("content", "").strip()
    
    # ë´‡ ê°ì§€ í˜ì´ì§€ì˜ íŠ¹ì§•ì ì¸ í…ìŠ¤íŠ¸ í™•ì¸
    if title == "kr.investing.com":
        return True
    
    if "Verifying you are human" in content or "This may take a few seconds" in content:
        return True
    
    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
    if len(content) < 50:
        return True
    
    return False


def load_json_file(json_path: str) -> List[Dict]:
    """JSON íŒŒì¼ ë¡œë“œ"""
    json_path = Path(json_path)
    if not json_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if not isinstance(data, list):
        raise ValueError(f"JSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤: {json_path}")
    
    return data


def get_date(page):
    """ë‚ ì§œ ì¶”ì¶œ"""
    # time datetime ì†ì„±
    if page.locator("time").count() > 0:
        dt = page.locator("time").first.get_attribute("datetime")
        if dt:
            return dt.strip().split("T")[0] if "T" in dt else dt.strip()
    
    # ë©”íƒ€ íƒœê·¸
    for sel, attr in (
        ('meta[property="article:published_time"]', "content"),
        ('meta[name="date"]', "content"),
        ('meta[name="dc.date"]', "content"),
    ):
        if page.locator(sel).count() > 0:
            val = page.locator(sel).first.get_attribute(attr)
            if val:
                return val.strip().split("T")[0] if "T" in val else val.strip()
    
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def get_content(page):
    """ë³¸ë¬¸ ì¶”ì¶œ"""
    use_article = page.locator("#article").count() > 0
    base_selector = "#article" if use_article else "body"
    
    # êµ¬ë… ìœ ë„ ì„¹ì…˜ê³¼ ê´‘ê³  ì„¹ì…˜ ì•ˆì˜ p íƒœê·¸ëŠ” ì œì™¸
    paras = page.eval_on_selector_all(
        f"{base_selector} p",
        """els => {
            const excludeIds = ['contextual-subscription-hook', 'mid-article-hook'];
            const excludeDataTests = ['contextual-subscription-hook', 'ad-slot-visible'];
            const excludeClasses = ['ad_adgroup', 'ad_ad__II8vw'];
            
            return els
                .filter(el => {
                    let parent = el.parentElement;
                    let depth = 0;
                    while (parent && depth < 10) {
                        // ID ì²´í¬
                        if (parent.id && excludeIds.some(id => parent.id.includes(id))) return false;
                        // data-test ì²´í¬
                        const dataTest = parent.getAttribute('data-test');
                        if (dataTest && excludeDataTests.some(test => dataTest.includes(test))) return false;
                        // í´ë˜ìŠ¤ ì²´í¬
                        const className = parent.className;
                        if (className && typeof className === 'string' && excludeClasses.some(cls => className.includes(cls))) return false;
                        parent = parent.parentElement;
                        depth++;
                    }
                    return true;
                })
                .map(e => e.innerText.trim())
                .filter(Boolean);
        }""",
    )
    
    # í™•ì‹¤í•œ ê´‘ê³  í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
    cleaned = []
    for t in paras:
        # ê¸¸ì´ ì²´í¬
        if len(t) <= 20:
            continue
        # í™•ì‹¤í•œ ê´‘ê³  í‚¤ì›Œë“œë§Œ ì²´í¬
        if "ê´‘ê³ " in t or "Advertisement" in t or "ì œ3ì ê´‘ê³ " in t:
            continue
        # "Investing.com-" ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì²« ë¬¸ì¥ ì œì™¸
        if t.startswith("Investing.com-") or t.startswith("Investing.com "):
            continue
        # ë²ˆì—­ ì•ˆë‚´ ë¬¸êµ¬ ì œì™¸
        if "ì´ ê¸°ì‚¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ë„ì›€ì„ ë°›ì•„ ë²ˆì—­ëìŠµë‹ˆë‹¤" in t:
            continue
        cleaned.append(t)
    
    return "\n".join(cleaned).strip()


def parse_article(page, article_url: str, wait_for_content: bool = True):
    """ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©, ë³¸ë¬¸, ë‚ ì§œ ì¶”ì¶œ"""
    try:
        # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ì¦ê°€ (DOM ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°)
        page.goto(article_url, wait_until="domcontentloaded", timeout=60000)
        
        # ì§€ì—° ë¡œë”© ìœ ë„ë¥¼ ìœ„í•œ ìŠ¤í¬ë¡¤
        for _ in range(4):
            page.evaluate("window.scrollBy(0, 800);")
            time.sleep(0.1)
        
        # ë´‡ ê°ì§€ í˜ì´ì§€ì¸ì§€ í™•ì¸í•˜ê³  ì‹¤ì œ ì½˜í…ì¸ ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if wait_for_content:
            max_wait_time = 30  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
            wait_interval = 2  # 2ì´ˆë§ˆë‹¤ í™•ì¸
            waited = 0
            
            while waited < max_wait_time:
                time.sleep(wait_interval)
                waited += wait_interval
                
                # í˜ì´ì§€ ë‚´ìš© í™•ì¸
                page_title = (page.locator("h1").first.text_content() or page.title() or "").strip()
                page_content = page.content()
                
                # ë´‡ ê°ì§€ í˜ì´ì§€ê°€ ì•„ë‹Œì§€ í™•ì¸
                if "Verifying you are human" not in page_content and page_title != "kr.investing.com":
                    # ì‹¤ì œ ì½˜í…ì¸ ê°€ ìˆëŠ”ì§€ í™•ì¸
                    content = get_content(page)
                    if content and len(content) > 50:
                        break
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if waited % 6 == 0:  # 6ì´ˆë§ˆë‹¤ ì¶œë ¥
                    print(f"    â³ ë´‡ ê°ì§€ í˜ì´ì§€ ëŒ€ê¸° ì¤‘... ({waited}ì´ˆ)")
        else:
            time.sleep(1)
        
        title = (page.locator("h1").first.text_content() or page.title() or "").strip()
        date = get_date(page)
        content = get_content(page)
        
        return {"title": title, "content": content, "date": date, "url": article_url}
    except Exception as e:
        print(f"  âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ({type(e).__name__}): {str(e)[:100]}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ë°˜í™˜
        return {"title": "", "content": "", "date": "", "url": article_url}


def filter_failed_articles(json_path: str) -> List[Dict]:
    """JSON íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ í•­ëª©ë“¤ í•„í„°ë§"""
    print("=" * 70)
    print("ğŸ” í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ë‰´ìŠ¤ í•„í„°ë§ ì¤‘...")
    print("=" * 70)
    
    articles = load_json_file(json_path)
    print(f"   ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    failed_articles = []
    for article in articles:
        if is_bot_detected(article):
            url = article.get("url", "")
            if url:  # URLì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                failed_articles.append(article)
    
    print(f"   âŒ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ê¸°ì‚¬: {len(failed_articles)}ê°œ")
    print(f"   âœ… ì •ìƒ í¬ë¡¤ë§ëœ ê¸°ì‚¬: {len(articles) - len(failed_articles)}ê°œ")
    
    return failed_articles


def recrawl_urls(urls: List[str], json_path: str, max_retries: int = 3) -> List[Dict]:
    """URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í¬ë¡¤ë§í•˜ê³  í•˜ë‚˜ì”© ì¦‰ì‹œ ì €ì¥"""
    print("=" * 70)
    print(f"ğŸ”„ {len(urls)}ê°œ URL ì¬í¬ë¡¤ë§ ì‹œì‘...")
    print("=" * 70)
    
    results = []
    success_count = 0
    import random
    
    # ëœë¤í•œ User-Agent ëª©ë¡
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
    ]
    
    # Playwright ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©
    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì €ëŠ” í•œ ë²ˆë§Œ ì‹œì‘
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-gpu",
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-setuid-sandbox",
                "--disable-web-security",
            ]
        )
        
        for idx, url in enumerate(urls, 1):
            print(f"\n[{idx}/{len(urls)}] í¬ë¡¤ë§ ì¤‘: {url[:60]}...")
            
            # ê° ìš”ì²­ë§ˆë‹¤ ì™„ì „íˆ ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¿ í‚¤/ì„¸ì…˜ ì´ˆê¸°í™”)
            context = browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent=random.choice(user_agents),
                ignore_https_errors=True,
                storage_state=None,  # ì¿ í‚¤ë‚˜ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì—†ì´ ì‹œì‘
            )
            
            page = context.new_page()
            page.set_default_timeout(60000)
            
            # webdriver ì†ì„± ìˆ¨ê¸°ê¸° (ë” ê°•í™”)
            page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']});
                window.chrome = {runtime: {}};
            """)
            
            retry_count = 0
            success = False
            
            while retry_count < max_retries and not success:
                try:
                    # ì²« ì‹œë„ì—ì„œëŠ” ë´‡ ê°ì§€ í˜ì´ì§€ ëŒ€ê¸° í™œì„±í™”, ì¬ì‹œë„ì—ì„œëŠ” ë¹„í™œì„±í™”
                    article_data = parse_article(page, url, wait_for_content=(retry_count == 0))
                    
                    # í¬ë¡¤ë§ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    if article_data and article_data.get("content") and len(article_data.get("content", "")) > 50:
                        # ë´‡ ê°ì§€ í˜ì´ì§€ì¸ì§€ ë‹¤ì‹œ í™•ì¸
                        if not is_bot_detected(article_data):
                            article_data["crawled_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            results.append(article_data)
                            success_count += 1
                            print(f"  âœ… ì„±ê³µ: {article_data['title'][:50]}... (ë³¸ë¬¸ {len(article_data['content'])}ì)")
                            
                            # ì¦‰ì‹œ JSON íŒŒì¼ ì—…ë°ì´íŠ¸
                            update_json_file_single(json_path, article_data)
                            print(f"  ğŸ’¾ JSON íŒŒì¼ì— ì €ì¥ ì™„ë£Œ")
                            
                            success = True
                        else:
                            print(f"  âš ï¸ ì—¬ì „íˆ ë´‡ ê°ì§€ í˜ì´ì§€ì…ë‹ˆë‹¤. ì¬ì‹œë„ ì¤‘... ({retry_count + 1}/{max_retries})")
                            retry_count += 1
                            if retry_count < max_retries:
                                # ì¬ì‹œë„ ì „ì— í˜ì´ì§€ë¥¼ ë‹«ê³  ìƒˆë¡œ ì—´ê¸°
                                page.close()
                                page = context.new_page()
                                page.set_default_timeout(60000)
                                page.add_init_script("""
                                    Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                                    Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                                    Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']});
                                    window.chrome = {runtime: {}};
                                """)
                                time.sleep(1)
                    else:
                        print(f"  âš ï¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¬ì‹œë„ ì¤‘... ({retry_count + 1}/{max_retries})")
                        retry_count += 1
                        if retry_count < max_retries:
                            page.close()
                            page = context.new_page()
                            page.set_default_timeout(60000)
                            page.add_init_script("""
                                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']});
                                window.chrome = {runtime: {}};
                            """)
                            time.sleep(1)
                            
                except Exception as e:
                    retry_count += 1
                    print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ ({type(e).__name__}): {str(e)[:100]}")
                    if retry_count < max_retries:
                        try:
                            page.close()
                            page = context.new_page()
                            page.set_default_timeout(60000)
                            page.add_init_script("""
                                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko', 'en-US', 'en']});
                                window.chrome = {runtime: {}};
                            """)
                        except:
                            pass
                        time.sleep(1)
            
            # ì»¨í…ìŠ¤íŠ¸ ì™„ì „íˆ ì¢…ë£Œ (ì„¸ì…˜ ì´ˆê¸°í™”)
            try:
                page.close()
                context.close()
            except:
                pass
            
            if not success:
                print(f"  âŒ ìµœì¢… ì‹¤íŒ¨: {url}")
            
            # ìš”ì²­ ê°„ ì§§ì€ ëŒ€ê¸° (ì„¸ì…˜ ê²©ë¦¬ë¥¼ ìœ„í•´)
            if idx < len(urls):  # ë§ˆì§€ë§‰ì´ ì•„ë‹ ë•Œë§Œ ëŒ€ê¸°
                wait_time = random.uniform(0.5, 1.5)  # 0.5~1.5ì´ˆ ëœë¤ ëŒ€ê¸°
                print(f"  â¸ï¸  {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(wait_time)
        
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        try:
            browser.close()
        except:
            pass
    
    print(f"\nâœ… ì¬í¬ë¡¤ë§ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ")
    return results


def update_json_file_single(json_path: str, recrawled_article: Dict):
    """ë‹¨ì¼ ì¬í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ì¦‰ì‹œ JSON íŒŒì¼ì— ì—…ë°ì´íŠ¸"""
    json_path_obj = Path(json_path)
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    try:
        all_articles = load_json_file(json_path)
    except:
        all_articles = []
    
    # URLì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
    url_to_article = {article.get("url"): article for article in all_articles}
    
    # ì¬í¬ë¡¤ë§ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
    url = recrawled_article.get("url")
    if url:
        if url in url_to_article:
            # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
            url_to_article[url].update(recrawled_article)
        else:
            # ìƒˆ í•­ëª© ì¶”ê°€
            url_to_article[url] = recrawled_article
    
    # ì—…ë°ì´íŠ¸ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    updated_articles = list(url_to_article.values())
    
    # íŒŒì¼ ì €ì¥
    with open(json_path_obj, "w", encoding="utf-8") as f:
        json.dump(updated_articles, f, ensure_ascii=False, indent=2)


def update_json_file(json_path: str, recrawled_articles: List[Dict]):
    """ì¬í¬ë¡¤ë§ëœ ë°ì´í„°ë¡œ JSON íŒŒì¼ ì—…ë°ì´íŠ¸ (ì¼ê´„ ì²˜ë¦¬ìš©)"""
    print("=" * 70)
    print("ğŸ’¾ JSON íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘...")
    print("=" * 70)
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    all_articles = load_json_file(json_path)
    
    # URLì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
    url_to_article = {article.get("url"): article for article in all_articles}
    
    # ì¬í¬ë¡¤ë§ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
    updated_count = 0
    for recrawled in recrawled_articles:
        url = recrawled.get("url")
        if url and url in url_to_article:
            # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
            url_to_article[url].update(recrawled)
            updated_count += 1
            print(f"  âœ… ì—…ë°ì´íŠ¸: {recrawled['title'][:50]}...")
    
    # ì—…ë°ì´íŠ¸ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    updated_articles = list(url_to_article.values())
    
    # íŒŒì¼ ì €ì¥
    json_path_obj = Path(json_path)
    
    # ìƒˆ íŒŒì¼ ì €ì¥
    with open(json_path_obj, "w", encoding="utf-8") as f:
        json.dump(updated_articles, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… JSON íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê°œ í•­ëª© ì—…ë°ì´íŠ¸")
    print(f"   ì´ {len(updated_articles)}ê°œ ê¸°ì‚¬")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ë‰´ìŠ¤ë¥¼ ì¬í¬ë¡¤ë§í•˜ê³  JSON íŒŒì¼ ì—…ë°ì´íŠ¸")
    parser.add_argument(
        "--json-path",
        type=str,
        default="data/investing_news.json",
        help="investing_news.json íŒŒì¼ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    
    # JSON íŒŒì¼ ê²½ë¡œ í™•ì¸
    json_path = Path(args.json_path)
    if not json_path.is_absolute():
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        script_dir = Path(__file__).parent
        json_path = script_dir / json_path
    
    if not json_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return
    
    print("=" * 70)
    print("ğŸš€ í¬ë¡¤ë§ ì‹¤íŒ¨ ë‰´ìŠ¤ ì¬ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print("=" * 70)
    print(f"ğŸ“ JSON íŒŒì¼: {json_path}")
    print()
    
    # 1. ì‹¤íŒ¨í•œ ê¸°ì‚¬ í•„í„°ë§
    failed_articles = filter_failed_articles(str(json_path))
    
    if not failed_articles:
        print("\nâœ… í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. URL ì¶”ì¶œ
    urls = [article.get("url") for article in failed_articles if article.get("url")]
    print(f"\nğŸ“‹ ì¬í¬ë¡¤ë§ ëŒ€ìƒ URL: {len(urls)}ê°œ")
    
    # 3. ì¬í¬ë¡¤ë§ (í•˜ë‚˜ì”© ì¦‰ì‹œ ì €ì¥)
    recrawled_articles = recrawl_urls(urls, str(json_path))
    
    if not recrawled_articles:
        print("\nâŒ ì¬í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   (ì¼ë¶€ ì„±ê³µí•œ í•­ëª©ì€ ì´ë¯¸ JSON íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤)")
        return
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 70)
    print(f"   ì¬í¬ë¡¤ë§ ì„±ê³µ: {len(recrawled_articles)}ê°œ")
    print("=" * 70)


if __name__ == "__main__":
    main()

