# -*- coding: utf-8 -*-

"""
Investing.com ë‰´ìŠ¤ ë°ì´í„°ë¥¼ MongoDBì— ì €ì¥í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
- ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
- í˜ë¥´ì†Œë‚˜ 5ëª… ë¶„ì„ (FastAPI prompts ì‚¬ìš©)
- ì˜í–¥ ë¯¸ì¹  ê¸°ì—… ëª©ë¡ ì¶”ì¶œ
- ì¤‘ë³µ ì œê±° ë¡œì§
"""

import os
import sys
import json
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

# FastAPI ì„œë²„ëŠ” HTTPë¡œ í˜¸ì¶œ (ë…ë¦½ì  ìš´ì˜)

import pymongo
from pymongo import MongoClient
from pymongo.errors import DuplicateKeyError
from dotenv import load_dotenv
from pathlib import Path
import requests
import openai

# .env íŒŒì¼ ê²½ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ì‚¬ìš©)
# docker-composeì—ì„œ /opt/airflow/.envë¡œ ë§ˆìš´íŠ¸ë¨
# override=True: ê¸°ì¡´ í™˜ê²½ ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì˜ ê°’ìœ¼ë¡œ ë®ì–´ì”€
env_path = Path("/opt/airflow/.env")
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
else:
    # ì ˆëŒ€ ê²½ë¡œì—ì„œë„ ì‹œë„
    env_path_abs = Path("/opt/S13P31B205/ai-service/.env")
    if env_path_abs.exists():
        load_dotenv(dotenv_path=env_path_abs, override=True)
    else:
        # ê¸°ë³¸ ê²½ë¡œì—ì„œë„ ì‹œë„
        load_dotenv(override=True)

# ============================================================================
# í™˜ê²½ ë³€ìˆ˜
# ============================================================================

# MONGODB_HOSTëŠ” docker-composeì—ì„œ ì„¤ì •ë˜ì§€ë§Œ, ê¸°ë³¸ê°’ì´ mongodbì¼ ìˆ˜ ìˆìŒ
# ì‹¤ì œ ì»¨í…Œì´ë„ˆ ì´ë¦„ì€ dollar-insight-mongodbì´ë¯€ë¡œ .env íŒŒì¼ì—ì„œ ì½ë„ë¡ í•¨
MONGODB_HOST = os.getenv("MONGODB_HOST", "dollar-insight-mongodb")
MONGODB_PORT = int(os.getenv("MONGODB_PORT", "27017"))
MONGODB_DB = os.getenv("MONGODB_DB", "dollar_insight")
# ì»¬ë ‰ì…˜ 2ê°œ: ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´, í˜ë¥´ì†Œë‚˜ ë¶„ì„
MONGODB_NEWS_COLLECTION = os.getenv("MONGODB_NEWS_COLLECTION", "investing_news")
MONGODB_PERSONA_COLLECTION = os.getenv(
    "MONGODB_PERSONA_COLLECTION", "news_persona_analysis"
)
# MongoDB ì¸ì¦ ì •ë³´ (ì„ íƒì‚¬í•­)
# .env íŒŒì¼ì˜ MONGODB_USER, MONGODB_PASSWORD ë˜ëŠ” MONGO_USER, MONGO_PASSWORD ì‚¬ìš©
# docker-compose-airflow.ymlì—ì„œ MONGO_USER, MONGO_PASSWORDë¡œ ì„¤ì •ë˜ë¯€ë¡œ ë‘˜ ë‹¤ í™•ì¸
# strip()ìœ¼ë¡œ ê°œí–‰ ë¬¸ì ì œê±°
_mongodb_user = os.getenv("MONGODB_USER") or os.getenv("MONGODB_USERNAME") or os.getenv("MONGO_USER")
_mongodb_pass = os.getenv("MONGODB_PASSWORD") or os.getenv("MONGO_PASSWORD")
# ë¹ˆ ë¬¸ìì—´ë„ Noneìœ¼ë¡œ ì²˜ë¦¬ (ì¸ì¦ ì—†ì´ ì—°ê²° ì‹œë„ ë°©ì§€)
MONGODB_USERNAME = (
    _mongodb_user.strip() if _mongodb_user and _mongodb_user.strip() else None
)
MONGODB_PASSWORD = (
    _mongodb_pass.strip() if _mongodb_pass and _mongodb_pass.strip() else None
)
MONGODB_AUTH_SOURCE = os.getenv("MONGODB_AUTH_SOURCE", "admin").strip()

# FastAPI ì„œë²„ ì„¤ì •
FASTAPI_URL = os.getenv("FASTAPI_URL", "http://localhost:8000")

# í˜ë¥´ì†Œë‚˜ ëª©ë¡
PERSONAS = ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]

# í˜ë¥´ì†Œë‚˜ ì´ë¦„ì„ ì˜ë¬¸ í•„ë“œëª…ìœ¼ë¡œ ë§¤í•‘
PERSONA_FIELD_MAP = {
    "í¬ì—´": "heuyeol",
    "ë•ìˆ˜": "deoksu",
    "ì§€ìœ¨": "jiyul",
    "í…Œì˜¤": "teo",
    "ë¯¼ì§€": "minji",
}

# FastAPIì—ì„œ ë°˜í™˜í•˜ëŠ” ì˜ë¬¸ í‚¤ë¥¼ í•œê¸€ë¡œ ë§¤í•‘
PERSONA_ENGLISH_TO_KOREAN = {
    "heuyeol": "í¬ì—´",
    "deoksu": "ë•ìˆ˜",
    "jiyul": "ì§€ìœ¨",
    "teo": "í…Œì˜¤",
    "minji": "ë¯¼ì§€",
}

# ì¶”ì  ëŒ€ìƒ ê¸°ì—…/ETF ëª©ë¡ (36ê°œ ê¸°ì—… + 14ê°œ ETF)
TRACKED_COMPANIES = [
    # ê¸°ìˆ  ê¸°ì—… (12ê°œ)
    "ì• í”Œ",
    "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "ì•„ë§ˆì¡´",
    "ë©”íƒ€",
    "ì—”ë¹„ë””ì•„",
    "AMD",
    "ì¸í…”",
    "TSMC",
    "ASML",
    "ì–´ë„ë¹„",
    "ì˜¤ë¼í´",
    # ì»¤ë¨¸ìŠ¤ (2ê°œ)
    "ì¿ íŒ¡",
    "ì•Œë¦¬ë°”ë°”",
    # ìë™ì°¨ (1ê°œ)
    "í…ŒìŠ¬ë¼",
    # í•­ê³µ (2ê°œ)
    "ë³´ì‰",
    "ë¸íƒ€í•­ê³µ",
    # ëª¨ë¹Œë¦¬í‹° (1ê°œ)
    "ìš°ë²„",
    # ì‚°ì—…/ë¬¼ë¥˜ (1ê°œ)
    "í˜ë±ìŠ¤",
    # ë¦¬í…Œì¼ (2ê°œ)
    "ì›”ë§ˆíŠ¸",
    "ì½”ìŠ¤íŠ¸ì½”",
    # ê¸ˆìœµ (3ê°œ)
    "JPëª¨ê±´",
    "BOA",
    "ê³¨ë“œë§Œì‚­ìŠ¤",
    # ê²°ì œ (3ê°œ)
    "ë¹„ì",
    "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "í˜ì´íŒ”",
    # ë³´í—˜ (1ê°œ)
    "AIG",
    # ì†Œë¹„ì¬ (5ê°œ)
    "ì½”ì¹´ì½œë¼",
    "í©ì‹œ",
    "ë§¥ë„ë‚ ë“œ",
    "ìŠ¤íƒ€ë²…ìŠ¤",
    "ë‚˜ì´í‚¤",
    # ë¯¸ë””ì–´/ì—”í„° (3ê°œ)
    "ë„·í”Œë¦­ìŠ¤",
    "ë””ì¦ˆë‹ˆ",
    "ì†Œë‹ˆ",
    # ETF (14ê°œ)
    "VOO",
    "SPY",
    "VTI",
    "QQQ",
    "QQQM",
    "TQQQ",
    "SCHD",
    "SOXX",
    "SMH",
    "ITA",
    "XLF",
    "XLY",
    "XLP",
    "ICLN",
]

# ê¸°ì—…ëª… ë§¤í•‘ (ì˜ì–´/ë‹¤ì–‘í•œ í‘œê¸° -> í•œê¸€)
COMPANY_NAME_MAPPING = {
    # ê¸°ìˆ  ê¸°ì—…
    "apple": "ì• í”Œ",
    "aapl": "ì• í”Œ",
    "microsoft": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "msft": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "google": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "alphabet": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "googl": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "goog": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "amazon": "ì•„ë§ˆì¡´",
    "amzn": "ì•„ë§ˆì¡´",
    "meta": "ë©”íƒ€",
    "facebook": "ë©”íƒ€",
    "fb": "ë©”íƒ€",
    "nvidia": "ì—”ë¹„ë””ì•„",
    "nvda": "ì—”ë¹„ë””ì•„",
    "amd": "AMD",
    "intel": "ì¸í…”",
    "intc": "ì¸í…”",
    "tsmc": "TSMC",
    "asml": "ASML",
    "adobe": "ì–´ë„ë¹„",
    "adbe": "ì–´ë„ë¹„",
    "oracle": "ì˜¤ë¼í´",
    "orcl": "ì˜¤ë¼í´",
    # ì»¤ë¨¸ìŠ¤
    "coupang": "ì¿ íŒ¡",
    "cpng": "ì¿ íŒ¡",
    "alibaba": "ì•Œë¦¬ë°”ë°”",
    "baba": "ì•Œë¦¬ë°”ë°”",
    # ìë™ì°¨
    "tesla": "í…ŒìŠ¬ë¼",
    "tsla": "í…ŒìŠ¬ë¼",
    # í•­ê³µ
    "boeing": "ë³´ì‰",
    "ba": "ë³´ì‰",
    "delta": "ë¸íƒ€í•­ê³µ",
    "dal": "ë¸íƒ€í•­ê³µ",
    # ëª¨ë¹Œë¦¬í‹°
    "uber": "ìš°ë²„",
    # ì‚°ì—…/ë¬¼ë¥˜
    "fedex": "í˜ë±ìŠ¤",
    "fdx": "í˜ë±ìŠ¤",
    # ë¦¬í…Œì¼
    "walmart": "ì›”ë§ˆíŠ¸",
    "wmt": "ì›”ë§ˆíŠ¸",
    "costco": "ì½”ìŠ¤íŠ¸ì½”",
    "cost": "ì½”ìŠ¤íŠ¸ì½”",
    # ê¸ˆìœµ
    "jpmorgan": "JPëª¨ê±´",
    "jpm": "JPëª¨ê±´",
    "jp morgan": "JPëª¨ê±´",
    "bank of america": "BOA",
    "boa": "BOA",
    "bac": "BOA",
    "goldman sachs": "ê³¨ë“œë§Œì‚­ìŠ¤",
    "gs": "ê³¨ë“œë§Œì‚­ìŠ¤",
    # ê²°ì œ
    "visa": "ë¹„ì",
    "v": "ë¹„ì",
    "mastercard": "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "ma": "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "paypal": "í˜ì´íŒ”",
    "pypl": "í˜ì´íŒ”",
    # ë³´í—˜
    "aig": "AIG",
    # ì†Œë¹„ì¬
    "coca-cola": "ì½”ì¹´ì½œë¼",
    "coca cola": "ì½”ì¹´ì½œë¼",
    "ko": "ì½”ì¹´ì½œë¼",
    "pepsi": "í©ì‹œ",
    "pep": "í©ì‹œ",
    "mcdonald": "ë§¥ë„ë‚ ë“œ",
    "mcdonalds": "ë§¥ë„ë‚ ë“œ",
    "mcd": "ë§¥ë„ë‚ ë“œ",
    "starbucks": "ìŠ¤íƒ€ë²…ìŠ¤",
    "sbux": "ìŠ¤íƒ€ë²…ìŠ¤",
    "nike": "ë‚˜ì´í‚¤",
    "nke": "ë‚˜ì´í‚¤",
    # ë¯¸ë””ì–´/ì—”í„°
    "netflix": "ë„·í”Œë¦­ìŠ¤",
    "nflx": "ë„·í”Œë¦­ìŠ¤",
    "disney": "ë””ì¦ˆë‹ˆ",
    "dis": "ë””ì¦ˆë‹ˆ",
    "sony": "ì†Œë‹ˆ",
    "sne": "ì†Œë‹ˆ",
}

# ê¸°ì—…ëª… -> í‹°ì»¤ ë§¤í•‘
COMPANY_TICKER_MAPPING = {
    # ê¸°ìˆ  ê¸°ì—…
    "ì• í”Œ": "AAPL",
    "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸": "MSFT",
    "êµ¬ê¸€(ì•ŒíŒŒë²³)": "GOOGL",
    "ì•„ë§ˆì¡´": "AMZN",
    "ë©”íƒ€": "META",
    "ì—”ë¹„ë””ì•„": "NVDA",
    "AMD": "AMD",
    "ì¸í…”": "INTC",
    "TSMC": "TSM",
    "ASML": "ASML",
    "ì–´ë„ë¹„": "ADBE",
    "ì˜¤ë¼í´": "ORCL",
    # ì»¤ë¨¸ìŠ¤
    "ì¿ íŒ¡": "CPNG",
    "ì•Œë¦¬ë°”ë°”": "BABA",
    # ìë™ì°¨
    "í…ŒìŠ¬ë¼": "TSLA",
    # í•­ê³µ
    "ë³´ì‰": "BA",
    "ë¸íƒ€í•­ê³µ": "DAL",
    # ëª¨ë¹Œë¦¬í‹°
    "ìš°ë²„": "UBER",
    # ì‚°ì—…/ë¬¼ë¥˜
    "í˜ë±ìŠ¤": "FDX",
    # ë¦¬í…Œì¼
    "ì›”ë§ˆíŠ¸": "WMT",
    "ì½”ìŠ¤íŠ¸ì½”": "COST",
    # ê¸ˆìœµ
    "JPëª¨ê±´": "JPM",
    "BOA": "BAC",
    "ê³¨ë“œë§Œì‚­ìŠ¤": "GS",
    # ê²°ì œ
    "ë¹„ì": "V",
    "ë§ˆìŠ¤í„°ì¹´ë“œ": "MA",
    "í˜ì´íŒ”": "PYPL",
    # ë³´í—˜
    "AIG": "AIG",
    # ì†Œë¹„ì¬
    "ì½”ì¹´ì½œë¼": "KO",
    "í©ì‹œ": "PEP",
    "ë§¥ë„ë‚ ë“œ": "MCD",
    "ìŠ¤íƒ€ë²…ìŠ¤": "SBUX",
    "ë‚˜ì´í‚¤": "NKE",
    # ë¯¸ë””ì–´/ì—”í„°
    "ë„·í”Œë¦­ìŠ¤": "NFLX",
    "ë””ì¦ˆë‹ˆ": "DIS",
    "ì†Œë‹ˆ": "SONY",
    # ETF (ì´ë¯¸ í‹°ì»¤ê°€ ì´ë¦„)
    "VOO": "VOO",
    "SPY": "SPY",
    "VTI": "VTI",
    "QQQ": "QQQ",
    "QQQM": "QQQM",
    "TQQQ": "TQQQ",
    "SCHD": "SCHD",
    "SOXX": "SOXX",
    "SMH": "SMH",
    "ITA": "ITA",
    "XLF": "XLF",
    "XLY": "XLY",
    "XLP": "XLP",
    "ICLN": "ICLN",
}


# ============================================================================
# MongoDB ì—°ê²°
# ============================================================================


def get_mongodb_client() -> MongoClient:
    """MongoDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì¸ì¦ ì§€ì›)"""
    if MONGODB_USERNAME and MONGODB_PASSWORD:
        # ì¸ì¦ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¸ì¦ í¬í•¨ (URL ì¸ì½”ë”© ì ìš©)
        from urllib.parse import quote_plus

        username = quote_plus(str(MONGODB_USERNAME))
        password = quote_plus(str(MONGODB_PASSWORD))
        connection_string = f"mongodb://{username}:{password}@{MONGODB_HOST}:{MONGODB_PORT}/{MONGODB_DB}?authSource={MONGODB_AUTH_SOURCE}"
        return MongoClient(connection_string)
    else:
        # ì¸ì¦ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì—°ê²°
        return MongoClient(MONGODB_HOST, MONGODB_PORT)


def get_mongodb_news_collection(client: MongoClient = None):
    """ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°"""
    if client is None:
        client = get_mongodb_client()
    db = client[MONGODB_DB]
    collection = db[MONGODB_NEWS_COLLECTION]

    # URLì„ ê¸°ì¤€ìœ¼ë¡œ unique ì¸ë±ìŠ¤ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
    collection.create_index("url", unique=True)

    return collection


def get_mongodb_persona_collection(client: MongoClient = None):
    """ë‰´ìŠ¤ í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°"""
    if client is None:
        client = get_mongodb_client()
    db = client[MONGODB_DB]
    collection = db[MONGODB_PERSONA_COLLECTION]

    # news_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ unique ì¸ë±ìŠ¤ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
    collection.create_index("news_id", unique=True)
    # news_urlë„ ì¸ë±ìŠ¤ë¡œ ìœ ì§€ (ê¸°ì¡´ í˜¸í™˜ì„± ë° ê²€ìƒ‰ìš©)
    collection.create_index("news_url")

    return collection


# ============================================================================
# LLMì„ í†µí•œ ê´€ë ¨ ê¸°ì—… ì¶”ì¶œ
# ============================================================================


def extract_related_companies_with_llm(title: str, content: str) -> List[str]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ì—ì„œ ê´€ë ¨ ê¸°ì—…/ETF ì¶”ì¶œ (ìµœëŒ€ 5ê°œ)

    Args:
        title: ë‰´ìŠ¤ ì œëª©
        content: ë‰´ìŠ¤ ë³¸ë¬¸

    Returns:
        ê´€ë ¨ ê¸°ì—…/ETF ëª©ë¡ (ìµœëŒ€ 5ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    OPENAI_API_KEY = os.getenv("GMS_API_KEY")
    GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"

    if not OPENAI_API_KEY:
        print("âš ï¸ GMS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ì—… ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=GMS_BASE_URL)

        # ì¶”ì  ëŒ€ìƒ ê¸°ì—… ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        tracked_companies_str = ", ".join(TRACKED_COMPANIES)

        # LLM í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì§ì ‘ ì–¸ê¸‰ë˜ê±°ë‚˜ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ì—…/ETFë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

## ë‰´ìŠ¤ ê¸°ì‚¬
ì œëª©: {title}
ë‚´ìš©: {content[:2000]}

## ì¶”ì  ëŒ€ìƒ ê¸°ì—…/ETF ëª©ë¡
{tracked_companies_str}

## ìš”ì²­
ìœ„ ëª©ë¡ ì¤‘ì—ì„œ ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ë˜ê±°ë‚˜ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ì—…/ETFë§Œ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 5ê°œ).
ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ëœ ê¸°ì—…ì´ ì—†ê±°ë‚˜ ê´€ë ¨ì´ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”.
ì •í™•í•˜ê²Œ ë§¤ì¹­ë˜ëŠ” ê¸°ì—…ë§Œ ì„ íƒí•˜ê³ , í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

## ì‘ë‹µ í˜•ì‹
{{"companies": ["ì• í”Œ", "í…ŒìŠ¬ë¼"]}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant that extracts company names from news. Always respond in valid JSON format only. Be precise and only include companies that are clearly mentioned or directly affected.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=200,
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        result = json.loads(response.choices[0].message.content.strip())
        extracted_companies = result.get("companies", [])

        # ê¸°ì—…ëª… ë§¤ì¹­ (ì •í™•ë„ í–¥ìƒ)
        matched_companies = []
        tracked_lower = {c.lower(): c for c in TRACKED_COMPANIES}

        for company in extracted_companies:
            company_clean = company.strip()
            company_lower = company_clean.lower()

            # ì§ì ‘ ë§¤ì¹­
            if company_lower in tracked_lower:
                matched = tracked_lower[company_lower]
                if matched not in matched_companies:
                    matched_companies.append(matched)
            # ë§¤í•‘ í…Œì´ë¸” í™•ì¸
            elif company_lower in COMPANY_NAME_MAPPING:
                matched = COMPANY_NAME_MAPPING[company_lower]
                if matched not in matched_companies:
                    matched_companies.append(matched)

        return matched_companies[:5]

    except Exception as e:
        print(f"âš ï¸ LLM ê¸°ì—… ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return []


# ============================================================================
# FastAPI ì„œë²„ë¥¼ í†µí•œ ë‰´ìŠ¤ ë¶„ì„
# ============================================================================


def analyze_news_via_api(title: str, content: str) -> Dict:
    """
    FastAPI ì„œë²„ë¥¼ í†µí•´ ë‰´ìŠ¤ ë¶„ì„ (í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ëª¨ë“  ë¶„ì„ ìˆ˜í–‰)

    Returns:
        {
            "summary": str,
            "persona_analyses": {persona: analysis},
            "companies": [str]
        }
    """
    try:
        response = requests.post(
            f"{FASTAPI_URL}/analyze-news",
            json={"title": title, "content": content},
            timeout=60,  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
        )

        if response.status_code == 200:
            return response.json()
        else:
            print(f"âš ï¸ FastAPI ì˜¤ë¥˜ (HTTP {response.status_code}): {response.text}")
            return {
                "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
                "persona_analyses": {p: f"{p} ë¶„ì„ ì‹¤íŒ¨" for p in PERSONAS},
                "companies": [],
            }
    except requests.exceptions.ConnectionError:
        print(f"âš ï¸ FastAPI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FASTAPI_URL}")
        print("   FastAPI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return {
            "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨ (ì„œë²„ ì—°ê²° ì‹¤íŒ¨)",
            "persona_analyses": {
                p: f"{p} ë¶„ì„ ì‹¤íŒ¨ (ì„œë²„ ì—°ê²° ì‹¤íŒ¨)" for p in PERSONAS
            },
            "companies": [],
        }
    except Exception as e:
        print(f"âš ï¸ FastAPI í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return {
            "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
            "persona_analyses": {p: f"{p} ë¶„ì„ ì‹¤íŒ¨" for p in PERSONAS},
            "companies": [],
        }


# ============================================================================
# ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
# ============================================================================


def process_news_article(article: Dict) -> tuple:
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ ê°€ê³µ (2ê°œ ì»¬ë ‰ì…˜ìœ¼ë¡œ ë¶„ë¦¬)

    Returns:
        (news_data, persona_data) íŠœí”Œ
        - news_data: ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´
        - persona_data: í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì •ë³´ (news_idëŠ” ì €ì¥ ì‹œ ì¶”ê°€ë¨)
    """
    title = article.get("title", "")
    content = article.get("content", "")
    url = article.get("url", "")
    date = article.get("date", "")
    crawled_at = article.get("crawled_at", "")

    print(f"ğŸ“° ì²˜ë¦¬ ì¤‘: {title[:50]}...")

    # FastAPI ì„œë²„ë¥¼ í†µí•´ í•œ ë²ˆì— ëª¨ë“  ë¶„ì„ ìˆ˜í–‰
    analysis_result = analyze_news_via_api(title, content)

    summary = analysis_result.get("summary", "ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
    # FastAPIëŠ” ì˜ë¬¸ í‚¤ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ í•œê¸€ë¡œ ë³€í™˜
    persona_analyses_english = analysis_result.get("persona_analyses", {})
    persona_analyses = {
        PERSONA_ENGLISH_TO_KOREAN.get(eng_key, eng_key): value
        for eng_key, value in persona_analyses_english.items()
    }

    print(f"  âœ“ ìš”ì•½ ì™„ë£Œ")
    print(f"  âœ“ í˜ë¥´ì†Œë‚˜ 5ëª… ë¶„ì„ ì™„ë£Œ")

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ê¸°ì—… ì¶”ì¶œ (ìµœëŒ€ 5ê°œ)
    related_companies = extract_related_companies_with_llm(title, content)

    if related_companies:
        print(f"  âœ“ ê´€ë ¨ ê¸°ì—… ì¶”ì¶œ: {len(related_companies)}ê°œ - {related_companies}")
    else:
        print(f"  âœ“ ê´€ë ¨ ê¸°ì—… ì—†ìŒ")

    # ê´€ë ¨ ê¸°ì—…ì˜ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ë¹ˆ ë¬¸ìì—´ ì œì™¸)
    related_tickers = [
        COMPANY_TICKER_MAPPING.get(company)
        for company in related_companies
        if COMPANY_TICKER_MAPPING.get(company)
    ]

    # 1. ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ (ì»¬ë ‰ì…˜ 1)
    news_data = {
        "title": title,
        "content": content,
        "url": url,
        "date": date,
        "summary": summary,  # ìš”ì•½ ì¶”ê°€
        "related_companies": related_companies,  # ê´€ë ¨ ê¸°ì—…/ETF ëª©ë¡ ì¶”ê°€
        "ticker": related_tickers,  # ê´€ë ¨ ê¸°ì—…/ETF í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        "crawled_at": crawled_at,
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        # persona_analysis_idëŠ” ì €ì¥ í›„ ì—…ë°ì´íŠ¸ë¨
    }

    # 2. í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì •ë³´ (ì»¬ë ‰ì…˜ 2)
    # news_idëŠ” ì €ì¥ ì‹œ ë‰´ìŠ¤ì˜ _idë¡œ ì„¤ì •ë¨
    # ê° í˜ë¥´ì†Œë‚˜ë³„ë¡œ ë³„ë„ ì»¬ëŸ¼ ìƒì„± (persona_hyeolyeol, persona_deoksu, persona_jiyul, persona_teo, persona_minji)
    persona_data = {
        "news_url": url,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (ê²€ìƒ‰ìš©)
        "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        # news_idëŠ” ì €ì¥ ì‹œ ì¶”ê°€ë¨
    }

    # ê° í˜ë¥´ì†Œë‚˜ë³„ë¡œ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥ (ì˜ë¬¸ í•„ë“œëª… ì‚¬ìš©)
    for persona_name in PERSONAS:
        persona_field = f"persona_{PERSONA_FIELD_MAP[persona_name]}"
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        persona_data[persona_field] = persona_analyses.get(
            persona_name, f"{persona_name} ë¶„ì„ ì‹¤íŒ¨"
        )

    return news_data, persona_data


def save_to_mongodb(
    news_list: List[Dict],
    persona_list: List[Dict],
    news_collection=None,
    persona_collection=None,
) -> Dict[str, int]:
    """
    ê°€ê³µëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ 2ê°œ ì»¬ë ‰ì…˜ì— ì €ì¥ (ì¤‘ë³µ ì œê±°, _id ê¸°ë°˜ ì°¸ì¡°/ì—­ì°¸ì¡°)

    Args:
        news_list: ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        persona_list: í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (news_listì™€ ê°™ì€ ìˆœì„œ)
        news_collection: ë‰´ìŠ¤ ì»¬ë ‰ì…˜
        persona_collection: í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì»¬ë ‰ì…˜
    """
    if news_collection is None or persona_collection is None:
        client = get_mongodb_client()
        if news_collection is None:
            news_collection = get_mongodb_news_collection(client)
        if persona_collection is None:
            persona_collection = get_mongodb_persona_collection(client)
        need_close = True
    else:
        need_close = False

    stats = {
        "total": len(news_list),
        "news_inserted": 0,
        "news_duplicates": 0,
        "persona_inserted": 0,
        "persona_duplicates": 0,
        "errors": 0,
    }

    # ë‰´ìŠ¤ì™€ í˜ë¥´ì†Œë‚˜ë¥¼ ìŒìœ¼ë¡œ ì²˜ë¦¬ (ê°™ì€ ì¸ë±ìŠ¤)
    for idx, (news_data, persona_data) in enumerate(zip(news_list, persona_list)):
        news_id = None
        persona_analysis_id = None

        # 1. ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ ì €ì¥ (ë˜ëŠ” ê¸°ì¡´ ë‰´ìŠ¤ ì°¾ê¸°)
        try:
            # ì¤‘ë³µ ì²´í¬: URLë¡œ ê¸°ì¡´ ë‰´ìŠ¤ ì°¾ê¸°
            existing_news = news_collection.find_one({"url": news_data.get("url")})

            if existing_news:
                # ê¸°ì¡´ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ ê·¸ _id ì‚¬ìš©
                news_id = existing_news["_id"]
                stats["news_duplicates"] += 1
                print(
                    f"  âš ï¸ ë‰´ìŠ¤ ì¤‘ë³µ ê±´ë„ˆëœ€: {news_data['title'][:50]}... (ê¸°ì¡´ ID ì‚¬ìš©)"
                )
            else:
                # ìƒˆ ë‰´ìŠ¤ ì €ì¥
                result = news_collection.insert_one(news_data)
                news_id = result.inserted_id
                stats["news_inserted"] += 1
                print(
                    f"  âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ: {news_data['title'][:50]}... (ID: {news_id})"
                )
        except Exception as e:
            stats["errors"] += 1
            print(f"  âŒ ë‰´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            continue

        # 2. í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì •ë³´ ì €ì¥ (news_id í¬í•¨)
        if news_id:
            try:
                # persona_dataì— news_id ì¶”ê°€
                persona_data["news_id"] = news_id

                # ì¤‘ë³µ ì²´í¬: news_idë¡œ ê¸°ì¡´ í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì°¾ê¸°
                existing_persona = persona_collection.find_one({"news_id": news_id})

                if existing_persona:
                    # ê¸°ì¡´ í˜ë¥´ì†Œë‚˜ ë¶„ì„ì´ ìˆìœ¼ë©´ ê·¸ _id ì‚¬ìš©
                    persona_analysis_id = existing_persona["_id"]
                    stats["persona_duplicates"] += 1
                    print(
                        f"  âš ï¸ í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì¤‘ë³µ ê±´ë„ˆëœ€: {persona_data.get('news_url', 'N/A')} (ê¸°ì¡´ ID ì‚¬ìš©)"
                    )
                else:
                    # ìƒˆ í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì €ì¥
                    result = persona_collection.insert_one(persona_data)
                    persona_analysis_id = result.inserted_id
                    stats["persona_inserted"] += 1
                    print(
                        f"  âœ… í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì €ì¥ ì™„ë£Œ: {persona_data.get('news_url', 'N/A')} (ID: {persona_analysis_id})"
                    )

                # 3. ë‰´ìŠ¤ì— persona_analysis_id ì—­ì°¸ì¡° ì¶”ê°€
                if persona_analysis_id:
                    try:
                        news_collection.update_one(
                            {"_id": news_id},
                            {"$set": {"persona_analysis_id": persona_analysis_id}},
                        )
                        print(
                            f"  âœ… ë‰´ìŠ¤ì— í˜ë¥´ì†Œë‚˜ ë¶„ì„ ID ì—­ì°¸ì¡° ì¶”ê°€: {persona_analysis_id}"
                        )
                    except Exception as e:
                        print(f"  âš ï¸ ë‰´ìŠ¤ ì—­ì°¸ì¡° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            except Exception as e:
                stats["errors"] += 1
                print(f"  âŒ í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì €ì¥ ì˜¤ë¥˜: {str(e)}")

    if need_close:
        client.close()

    return stats


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================


def load_investing_news_json(json_path: str) -> List[Dict]:
    """investing_news.json íŒŒì¼ ë¡œë“œ"""
    json_path = Path(json_path)
    if not json_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if not isinstance(data, list):
        raise ValueError(f"JSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤: {json_path}")

    return data


def process_and_save(
    json_path: str, news_collection=None, persona_collection=None
) -> Dict[str, int]:
    """
    ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰: JSON ë¡œë“œ â†’ ê°€ê³µ â†’ MongoDB ì €ì¥ (2ê°œ ì»¬ë ‰ì…˜)

    Args:
        json_path: investing_news.json íŒŒì¼ ê²½ë¡œ
        news_collection: ë‰´ìŠ¤ ì»¬ë ‰ì…˜ (Noneì´ë©´ ìƒˆë¡œ ì—°ê²°)
        persona_collection: í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì»¬ë ‰ì…˜ (Noneì´ë©´ ìƒˆë¡œ ì—°ê²°)
    """
    print("=" * 70)
    print("ğŸ“Š Investing.com ë‰´ìŠ¤ MongoDB ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    print("=" * 70)

    # 1. JSON íŒŒì¼ ë¡œë“œ
    print(f"\n1ï¸âƒ£ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    articles = load_investing_news_json(json_path)
    print(f"   ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    # 2. MongoDB ì—°ê²°
    if news_collection is None or persona_collection is None:
        print(f"\n2ï¸âƒ£ MongoDB ì—°ê²° ì¤‘:")
        print(f"   - ë‰´ìŠ¤ ì»¬ë ‰ì…˜: {MONGODB_DB}/{MONGODB_NEWS_COLLECTION}")
        print(f"   - í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì»¬ë ‰ì…˜: {MONGODB_DB}/{MONGODB_PERSONA_COLLECTION}")
        client = get_mongodb_client()
        if news_collection is None:
            news_collection = get_mongodb_news_collection(client)
        if persona_collection is None:
            persona_collection = get_mongodb_persona_collection(client)
        need_close = True
    else:
        need_close = False

    # 3. ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ì¤‘ë³µ ì²´í¬)
    existing_news_urls = set(news_collection.distinct("url"))
    existing_persona_news_ids = set(
        str(doc["news_id"])
        for doc in persona_collection.find({}, {"news_id": 1})
        if "news_id" in doc
    )
    print(f"   ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°: {len(existing_news_urls)}ê°œ")
    print(f"   ê¸°ì¡´ í˜ë¥´ì†Œë‚˜ ë¶„ì„: {len(existing_persona_news_ids)}ê°œ")

    # 4. ìƒˆ ê¸°ì‚¬ë§Œ í•„í„°ë§ (ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ í˜ë¥´ì†Œë‚˜ ë¶„ì„ì´ ì—†ëŠ” ê²½ìš°)
    new_articles = []
    for article in articles:
        url = article.get("url", "")
        if url:
            # ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜, ë‰´ìŠ¤ëŠ” ìˆì§€ë§Œ í˜ë¥´ì†Œë‚˜ ë¶„ì„ì´ ì—†ëŠ” ê²½ìš°
            if url not in existing_news_urls:
                new_articles.append(article)
            else:
                # ë‰´ìŠ¤ëŠ” ìˆì§€ë§Œ í˜ë¥´ì†Œë‚˜ ë¶„ì„ì´ ì—†ëŠ”ì§€ í™•ì¸
                existing_news = news_collection.find_one({"url": url})
                if existing_news and "persona_analysis_id" not in existing_news:
                    new_articles.append(article)

    print(f"   ì‹ ê·œ/ì—…ë°ì´íŠ¸ ê¸°ì‚¬: {len(new_articles)}ê°œ")

    if not new_articles:
        print("\nâœ… ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        if need_close:
            client.close()
        return {
            "total": len(articles),
            "new": 0,
            "processed": 0,
            "news_inserted": 0,
            "news_duplicates": 0,
            "persona_inserted": 0,
            "persona_duplicates": 0,
            "errors": 0,
        }

    # 5. ê° ê¸°ì‚¬ ê°€ê³µ
    print(f"\n3ï¸âƒ£ ë‰´ìŠ¤ ê°€ê³µ ì¤‘ ({len(new_articles)}ê°œ)...")
    news_list = []
    persona_list = []
    for idx, article in enumerate(new_articles, 1):
        print(f"\n[{idx}/{len(new_articles)}]")
        try:
            news_data, persona_data = process_news_article(article)
            news_list.append(news_data)
            persona_list.append(persona_data)
        except Exception as e:
            print(f"  âŒ ê°€ê³µ ì‹¤íŒ¨: {str(e)}")
            continue

    # 6. MongoDBì— ì €ì¥ (2ê°œ ì»¬ë ‰ì…˜)
    print(f"\n4ï¸âƒ£ MongoDB ì €ì¥ ì¤‘...")
    print(f"   - ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´: {len(news_list)}ê°œ")
    print(f"   - í˜ë¥´ì†Œë‚˜ ë¶„ì„: {len(persona_list)}ê°œ")
    stats = save_to_mongodb(
        news_list, persona_list, news_collection, persona_collection
    )

    if need_close:
        client.close()

    # 7. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
    print("=" * 70)
    print(f"ì „ì²´ ê¸°ì‚¬: {len(articles)}ê°œ")
    print(f"ì‹ ê·œ ê¸°ì‚¬: {len(new_articles)}ê°œ")
    print(f"ê°€ê³µ ì™„ë£Œ: {len(news_list)}ê°œ")
    print(f"\në‰´ìŠ¤ ê¸°ë³¸ ì •ë³´:")
    print(f"  - ì €ì¥ ì„±ê³µ: {stats['news_inserted']}ê°œ")
    print(f"  - ì¤‘ë³µ ê±´ë„ˆëœ€: {stats['news_duplicates']}ê°œ")
    print(f"\ní˜ë¥´ì†Œë‚˜ ë¶„ì„:")
    print(f"  - ì €ì¥ ì„±ê³µ: {stats['persona_inserted']}ê°œ")
    print(f"  - ì¤‘ë³µ ê±´ë„ˆëœ€: {stats['persona_duplicates']}ê°œ")
    print(f"\nì˜¤ë¥˜ ë°œìƒ: {stats['errors']}ê°œ")
    print("=" * 70)

    return {
        "total": len(articles),
        "new": len(new_articles),
        "processed": len(news_list),
        **stats,
    }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # JSON íŒŒì¼ ê²½ë¡œ (AI_airflow/data/investing_news.json)
    current_dir = Path(__file__).resolve().parent
    json_path = current_dir.parent / "data" / "investing_news.json"

    if not json_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return

    # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    stats = process_and_save(str(json_path))

    print("\nâœ… í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    print(json.dumps(stats, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
