# -*- coding: utf-8 -*-

"""
MongoDBì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ kssì™€ bge-m3ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°í™”í•˜ì—¬ ChromaDBì— ì €ì¥
- KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬ í›„, 400ì ì²­í¬ë¡œ ë¬¶ê¸° (100ì overlap)
- ê° ì²­í¬ì— ì œëª© í¬í•¨
- í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì œì™¸
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import time

# Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ê¶Œí•œ ë¬¸ì œ í•´ê²°)
# Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì“°ê¸° ê°€ëŠ¥í•œ ìœ„ì¹˜ë¡œ ì„¤ì •
if not os.getenv("HF_HOME"):
    os.environ["HF_HOME"] = "/opt/airflow/.cache/huggingface"
if not os.getenv("TRANSFORMERS_CACHE"):
    os.environ["TRANSFORMERS_CACHE"] = "/opt/airflow/.cache/huggingface"
if not os.getenv("HF_DATASETS_CACHE"):
    os.environ["HF_DATASETS_CACHE"] = "/opt/airflow/.cache/huggingface"

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±)
cache_dir = os.environ.get("HF_HOME", "/opt/airflow/.cache/huggingface")
os.makedirs(cache_dir, exist_ok=True)
# ê¶Œí•œ ë³€ê²½ ì‹œë„ (ê¶Œí•œì´ ì—†ìœ¼ë©´ ë¬´ì‹œ)
try:
    os.chmod(cache_dir, 0o755)
except PermissionError:
    # ê¶Œí•œì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰ (ë””ë ‰í† ë¦¬ëŠ” ì´ë¯¸ ìƒì„±ë¨)
    pass

# ChromaDB ì§ì ‘ ì—°ê²° (FastAPI ì˜ì¡´ì„± ì œê±°)
from chromadb import HttpClient
from chromadb.config import Settings

import pymongo
from pymongo import MongoClient
from dotenv import load_dotenv
from pathlib import Path
import kss
from FlagEmbedding import FlagModel

# .env íŒŒì¼ ê²½ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ì‚¬ìš©)
# docker-composeì—ì„œ /opt/airflow/.envë¡œ ë§ˆìš´íŠ¸ë¨
# override=True: ê¸°ì¡´ í™˜ê²½ ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì˜ ê°’ìœ¼ë¡œ ë®ì–´ì”€
env_path = Path("/opt/airflow/.env")
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
else:
    # ì ˆëŒ€ ê²½ë¡œì—ì„œë„ ì‹œë„
    env_path_abs = Path("/opt/S13P31B205/ai-service/.env")
    if env_path_abs.exists():
        load_dotenv(dotenv_path=env_path_abs, override=True)
    else:
        # ê¸°ë³¸ ê²½ë¡œì—ì„œë„ ì‹œë„
        load_dotenv(override=True)

# ============================================================================
# í™˜ê²½ ë³€ìˆ˜
# ============================================================================

# MONGODB_HOSTëŠ” docker-composeì—ì„œ ì„¤ì •ë˜ì§€ë§Œ, ê¸°ë³¸ê°’ì´ mongodbì¼ ìˆ˜ ìˆìŒ
# ì‹¤ì œ ì»¨í…Œì´ë„ˆ ì´ë¦„ì€ dollar-insight-mongodbì´ë¯€ë¡œ .env íŒŒì¼ì—ì„œ ì½ë„ë¡ í•¨
MONGODB_HOST = os.getenv("MONGODB_HOST", "dollar-insight-mongodb")
MONGODB_PORT = int(os.getenv("MONGODB_PORT", "27017"))
MONGODB_DB = os.getenv("MONGODB_DB", "dollar_insight")
# ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ ì»¬ë ‰ì…˜ ì‚¬ìš© (ë²¡í„°í™”ëŠ” ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ì—ì„œ)
MONGODB_NEWS_COLLECTION = os.getenv("MONGODB_NEWS_COLLECTION", "investing_news")
# MongoDB ì¸ì¦ ì •ë³´ (ì„ íƒì‚¬í•­)
# .env íŒŒì¼ì˜ MONGODB_USER, MONGODB_PASSWORD ë˜ëŠ” MONGO_USER, MONGO_PASSWORD ì‚¬ìš©
# docker-compose-airflow.ymlì—ì„œ MONGO_USER, MONGO_PASSWORDë¡œ ì„¤ì •ë˜ë¯€ë¡œ ë‘˜ ë‹¤ í™•ì¸
# strip()ìœ¼ë¡œ ê°œí–‰ ë¬¸ì ì œê±°
_mongodb_user = os.getenv("MONGODB_USER") or os.getenv("MONGODB_USERNAME") or os.getenv("MONGO_USER")
_mongodb_pass = os.getenv("MONGODB_PASSWORD") or os.getenv("MONGO_PASSWORD")
MONGODB_USERNAME = _mongodb_user.strip() if _mongodb_user and _mongodb_user.strip() else None
MONGODB_PASSWORD = _mongodb_pass.strip() if _mongodb_pass and _mongodb_pass.strip() else None
MONGODB_AUTH_SOURCE = os.getenv("MONGODB_AUTH_SOURCE", "admin").strip()

# ChromaDB ì„¤ì •
CHROMADB_URL = os.getenv("CHROMADB_URL", "3.34.50.3")
CHROMADB_PORT = int(os.getenv("CHROMADB_PORT", "9000"))
CHROMADB_COLLECTION_NAME = os.getenv("CHROMADB_COLLECTION_NAME", "news_bge_m3")

# BGE-M3 ëª¨ë¸ ì„¤ì •
BGE_M3_MODEL_NAME = "BAAI/bge-m3"
BGE_M3_MODEL_PATH = os.getenv("BGE_M3_MODEL_PATH", None)  # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)


# ============================================================================
# ChromaDB ì—°ê²°
# ============================================================================


def make_chroma_client():
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return HttpClient(
        host=CHROMADB_URL,
        port=CHROMADB_PORT,
        settings=Settings(anonymized_telemetry=False),
    )


# ============================================================================
# MongoDB ì—°ê²°
# ============================================================================


def get_mongodb_client() -> MongoClient:
    """MongoDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì¸ì¦ ì§€ì›)"""
    if MONGODB_USERNAME and MONGODB_PASSWORD:
        # ì¸ì¦ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¸ì¦ í¬í•¨ (URL ì¸ì½”ë”© ì ìš©)
        from urllib.parse import quote_plus
        username = quote_plus(str(MONGODB_USERNAME))
        password = quote_plus(str(MONGODB_PASSWORD))
        connection_string = f"mongodb://{username}:{password}@{MONGODB_HOST}:{MONGODB_PORT}/{MONGODB_DB}?authSource={MONGODB_AUTH_SOURCE}"
        return MongoClient(connection_string)
    else:
        # ì¸ì¦ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì—°ê²°
        return MongoClient(MONGODB_HOST, MONGODB_PORT)


def get_mongodb_news_collection(client: MongoClient = None):
    """ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° (ë²¡í„°í™”ìš©)"""
    if client is None:
        client = get_mongodb_client()
    db = client[MONGODB_DB]
    return db[MONGODB_NEWS_COLLECTION]


# ============================================================================
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (KSS + ì²­í¬ ë‹¨ìœ„ ë¶„í• )
# ============================================================================


def split_text_into_sentences(text: str, use_kss: bool = True) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    - ê¸°ë³¸ì ìœ¼ë¡œ KSS + Mecab ë°±ì—”ë“œ ì‚¬ìš© (ë¹ ë¥´ê³  ì •í™•í•¨)
    - use_kss=Falseì¼ ë•Œ ì •ê·œì‹ ê¸°ë°˜ ë¶„ë¦¬ ì‚¬ìš© (ë” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ)
    
    Args:
        text: ë¶„í• í•  í…ìŠ¤íŠ¸
        use_kss: KSS ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True - Mecab ë°±ì—”ë“œ ì‚¬ìš©)
    
    Returns:
        ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip():
        return []
    
    if use_kss:
        # KSS + Mecab ë°±ì—”ë“œ ì‚¬ìš© (ë¹ ë¥´ê³  ì •í™•í•¨)
        sentences = kss.split_sentences(text)
    else:
        # ë¹ ë¥¸ ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬ (í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ ê¸°ì¤€)
        import re
        # ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸: . ! ? (í•œêµ­ì–´/ì˜ë¬¸ ëª¨ë‘)
        # ì¤„ë°”ê¿ˆë„ ë¬¸ì¥ êµ¬ë¶„ìë¡œ ì‚¬ìš©
        pattern = r'([.!?ã€‚ï¼ï¼Ÿ]\s*|\n+)'
        parts = re.split(pattern, text)
        
        sentences = []
        current_sentence = ""
        for i, part in enumerate(parts):
            if re.match(pattern, part):
                # ì¢…ê²° ë¶€í˜¸ë‚˜ ì¤„ë°”ê¿ˆ ë°œê²¬
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""
            else:
                current_sentence += part
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ê°€
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
    
    # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ê³µë°± ì œê±°
    result = []
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and len(sentence) > 5:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
            result.append(sentence)
    
    return result


def create_chunks_from_sentences(sentences: List[str], chunk_size: int = 400, overlap_size: int = 100) -> List[str]:
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¬¶ê¸° (overlap ì ìš©)
    
    Args:
        sentences: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (KSSë¡œ ë¶„ë¦¬ëœ ê²ƒ)
        chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
        overlap_size: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (overlapì„ ìœ„í•´ í¬í•¨í•  ë¬¸ì ìˆ˜)
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if not sentences:
        return []
    
    chunks = []
    current_chunk = []
    current_length = 0
    overlap_sentences = []  # overlapì„ ìœ„í•œ ë¬¸ì¥ ì €ì¥
    
    for sentence in sentences:
        sentence_length = len(sentence)
        
        # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ì‹œ ì˜ˆìƒ ê¸¸ì´
        if current_chunk:
            # ê³µë°± í¬í•¨
            expected_length = current_length + sentence_length + 1
        else:
            expected_length = current_length + sentence_length
        
        # ì²­í¬ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ë©´ í˜„ì¬ ì²­í¬ ì €ì¥
        if expected_length > chunk_size and current_chunk:
            # í˜„ì¬ ì²­í¬ ì €ì¥
            chunk_text = " ".join(current_chunk)
            chunks.append(chunk_text)
            
            # Overlapì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ê°œ ë¬¸ì¥ì„ ë‹¤ìŒ ì²­í¬ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš©
            overlap_sentences = []
            overlap_length = 0
            
            # ë’¤ì—ì„œë¶€í„° overlap_sizeë§Œí¼ ë¬¸ì¥ ìˆ˜ì§‘
            for i in range(len(current_chunk) - 1, -1, -1):
                sent = current_chunk[i]
                if overlap_length + len(sent) <= overlap_size:
                    overlap_sentences.insert(0, sent)
                    overlap_length += len(sent) + 1  # ê³µë°± í¬í•¨
                else:
                    break
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ (overlap ë¬¸ì¥ í¬í•¨)
            current_chunk = overlap_sentences.copy()
            current_length = overlap_length - 1 if overlap_sentences else 0  # ë§ˆì§€ë§‰ ê³µë°± ì œê±°
            
            # í˜„ì¬ ë¬¸ì¥ ì¶”ê°€
            if current_chunk:
                current_chunk.append(sentence)
                current_length = current_length + sentence_length + 1
            else:
                current_chunk = [sentence]
                current_length = sentence_length
        else:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€
            current_chunk.append(sentence)
            current_length = expected_length
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
    if current_chunk:
        chunk_text = " ".join(current_chunk)
        chunks.append(chunk_text)
    
    return chunks


def prepare_text_for_embedding(article: Dict) -> List[str]:
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë³€í™˜
    - KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬ í›„, 400ì ì²­í¬ë¡œ ë¬¶ê¸° (100ì overlap)
    - ê° ì²­í¬ì— ì œëª© ë¶™ì´ê¸°
    - í˜ë¥´ì†Œë‚˜ ë¶„ì„ ì œì™¸
    
    Args:
        article: ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´ (title, content, summary í¬í•¨)
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ì— titleì´ í¬í•¨ë¨)
    """
    title = article.get("title", "")
    content = article.get("content", "")
    
    if not content or not content.strip():
        return []
    
    # 1. KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_text_into_sentences(content)
    
    if not sentences:
        return []
    
    # 2. ë¬¸ì¥ë“¤ì„ 400ì ì²­í¬ë¡œ ë¬¶ê¸° (100ì overlap)
    content_chunks = create_chunks_from_sentences(sentences, chunk_size=400, overlap_size=100)
    
    # 3. ê° ì²­í¬ì— title ë¶™ì´ê¸°
    chunks = []
    for chunk in content_chunks:
        if title:
            # title + ì²­í¬ ì¡°í•©
            combined = f"{title} {chunk}"
        else:
            combined = chunk
        
        chunks.append(combined)
    
    return chunks


# ============================================================================
# ë²¡í„°í™” (BGE-M3 ì‚¬ìš©)
# ============================================================================


class BGE_M3_Embedder:
    """BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: Optional[str] = None):
        """
        BGE-M3 ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_path: ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ)
        """
        print(f"ğŸ”„ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘...")
        try:
            if model_path and os.path.exists(model_path):
                self.model = FlagModel(model_path, use_fp16=True)
                print(f"âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            else:
                self.model = FlagModel(BGE_M3_MODEL_NAME, use_fp16=True)
                print(f"âœ… HuggingFace ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {BGE_M3_MODEL_NAME}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def encode(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì‘ì€ ë°°ì¹˜ ì‚¬ìš©)
        
        Returns:
            ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°)
        """
        if not texts:
            return []
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬
        # ì „ì²´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë  ìˆ˜ ìˆìŒ
        all_embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        print(f"   ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ {total_batches}ê°œ ë°°ì¹˜)")
        
        for i in range(0, len(texts), batch_size):
            batch_num = (i // batch_size) + 1
            batch = texts[i:i + batch_size]
            try:
                # ì§„í–‰ë¥  ì¶œë ¥ (10ë°°ì¹˜ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ ë°°ì¹˜)
                if batch_num % 10 == 0 or batch_num == total_batches:
                    print(f"      ë²¡í„°í™” ì§„í–‰ ì¤‘: {batch_num}/{total_batches} ë°°ì¹˜ ({i+1}~{min(i+len(batch), len(texts))}/{len(texts)}ê°œ ì²­í¬)")
                
                # BGE-M3 encode: dense ë²¡í„° ë°˜í™˜
                embeddings = self.model.encode(batch)
                
                # numpy arrayë¥¼ listë¡œ ë³€í™˜
                if hasattr(embeddings, 'tolist'):
                    embeddings = embeddings.tolist()
                
                all_embeddings.extend(embeddings)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ë¹„ì§€ ì»¬ë ‰ì…˜)
                if batch_num % 20 == 0:
                    import gc
                    gc.collect()
                    
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}): {str(e)}")
                import traceback
                traceback.print_exc()
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ë¹ˆ ë²¡í„°ë¡œ ì±„ì›€
                all_embeddings.extend([[0.0] * 1024] * len(batch))  # BGE-M3ëŠ” 1024ì°¨ì›
        
        print(f"   âœ… ë²¡í„°í™” ì™„ë£Œ: ì´ {len(all_embeddings)}ê°œ ë²¡í„° ìƒì„±")
        return all_embeddings


# ============================================================================
# ChromaDB ì €ì¥
# ============================================================================


def get_or_create_chromadb_collection(client, collection_name: str):
    """ChromaDB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
    try:
        collection = client.get_collection(collection_name)
        print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}")
    except Exception:
        # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "Investing.com ë‰´ìŠ¤ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (BGE-M3)"}
        )
        print(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
    
    return collection


def save_to_chromadb(
    articles: List[Dict],
    collection,
    embedder,  # FlagModel ì§ì ‘ ì‚¬ìš© (BGE_M3_Embedder ëŒ€ì‹ )
    batch_size: int = 50  # 10 -> 50ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ì €ì¥ ì†ë„ í–¥ìƒ
) -> Dict[str, int]:
    """
    ê°€ê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ChromaDBì— ë²¡í„°í™”í•˜ì—¬ ì €ì¥
    
    Args:
        articles: MongoDBì—ì„œ ê°€ì ¸ì˜¨ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        collection: ChromaDB ì»¬ë ‰ì…˜
        embedder: BGE-M3 ì„ë² ë”
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        í†µê³„ ì •ë³´
    """
    stats = {
        "total_articles": len(articles),
        "total_chunks": 0,
        "saved_chunks": 0,
        "errors": 0
    }
    
    # ê¸°ì¡´ ë¬¸ì„œ ID í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
    existing_ids = set()
    try:
        existing = collection.get()
        existing_ids = set(existing.get("ids", []))
    except Exception:
        pass
    
    print(f"   ê¸°ì¡´ ë²¡í„° ë°ì´í„°: {len(existing_ids)}ê°œ")
    
    # ê° ê¸°ì‚¬ ì²˜ë¦¬
    all_texts = []
    all_metadatas = []
    all_ids = []
    
    print(f"   ê¸°ì‚¬ ì „ì²˜ë¦¬ ì¤‘... (ì´ {len(articles)}ê°œ ê¸°ì‚¬)")
    preprocess_start = time.time()
    for idx, article in enumerate(articles, 1):
        if idx % 10 == 0 or idx == len(articles):
            elapsed = time.time() - preprocess_start
            print(f"      ì „ì²˜ë¦¬ ì§„í–‰ ì¤‘: {idx}/{len(articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
        article_id = str(article.get("_id", ""))
        url = article.get("url", "")
        
        # í…ìŠ¤íŠ¸ ì²­í¬ ì¤€ë¹„
        chunks = prepare_text_for_embedding(article)
        
        if not chunks:
            continue
        
        stats["total_chunks"] += len(chunks)
        
        # ê° ì²­í¬ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        for chunk_idx, chunk in enumerate(chunks):
            chunk_id = f"{article_id}_chunk_{chunk_idx}"
            
            # ì¤‘ë³µ ì²´í¬
            if chunk_id in existing_ids:
                continue
            
            all_texts.append(chunk)
            all_metadatas.append({
                "article_id": article_id,
                "url": url,
                "title": article.get("title", "")[:200],  # ChromaDB ë©”íƒ€ë°ì´í„° ê¸¸ì´ ì œí•œ
                "date": article.get("date", ""),
                "chunk_index": chunk_idx,
                "total_chunks": len(chunks),
            })
            all_ids.append(chunk_id)
    
    if not all_texts:
        print("   ì‹ ê·œ ë²¡í„°í™”í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return stats
    
    print(f"   ë²¡í„°í™”í•  ì²­í¬: {len(all_texts)}ê°œ")
    
    # ë²¡í„°í™” - Redditì²˜ëŸ¼ FlagModelì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
    vectorize_batch_size = 64  # Redditê³¼ ë™ì¼í•œ ë°°ì¹˜ í¬ê¸°
    print(f"   ë²¡í„°í™” ì¤‘... (ì´ {len(all_texts)}ê°œ ì²­í¬, ë°°ì¹˜ í¬ê¸°: {vectorize_batch_size})")
    vectorize_start = time.time()
    try:
        # FlagModelì˜ encodeë¥¼ ì§ì ‘ í˜¸ì¶œ (ë‚´ë¶€ ìµœì í™” í™œìš©, Redditê³¼ ë™ì¼í•œ ë°©ì‹)
        embeddings = embedder.encode(all_texts, batch_size=vectorize_batch_size)
        
        # numpy arrayë¥¼ listë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        if hasattr(embeddings, 'tolist'):
            embeddings = embeddings.tolist()
        vectorize_elapsed = time.time() - vectorize_start
        print(f"   âœ… ë²¡í„°í™” ì™„ë£Œ (ì†Œìš” ì‹œê°„: {vectorize_elapsed:.1f}ì´ˆ, í‰ê· : {vectorize_elapsed/len(all_texts)*1000:.1f}ms/ì²­í¬)")
        
        if len(embeddings) != len(all_texts):
            print(f"âš ï¸ ì„ë² ë”© ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(embeddings)} != {len(all_texts)}")
            stats["errors"] += len(all_texts) - len(embeddings)
        
        # ChromaDBì— ì €ì¥ (ë°°ì¹˜ ë‹¨ìœ„)
        total_save_batches = (len(all_texts) + batch_size - 1) // batch_size
        print(f"   ChromaDB ì €ì¥ ì¤‘... (ì´ {total_save_batches}ê°œ ë°°ì¹˜)")
        save_start = time.time()
        for i in range(0, len(all_texts), batch_size):
            batch_num = (i // batch_size) + 1
            batch_texts = all_texts[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_metadatas = all_metadatas[i:i + batch_size]
            batch_ids = all_ids[i:i + batch_size]
            
            try:
                print(f"      ì €ì¥ ì§„í–‰ ì¤‘: {batch_num}/{total_save_batches} ë°°ì¹˜ ({i+1}~{min(i+len(batch_ids), len(all_texts))}/{len(all_texts)}ê°œ ì²­í¬)")
                collection.add(
                    embeddings=batch_embeddings,
                    documents=batch_texts,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                stats["saved_chunks"] += len(batch_ids)
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}): {str(e)}")
                stats["errors"] += len(batch_ids)
        
        save_elapsed = time.time() - save_start
        print(f"   âœ… ChromaDB ì €ì¥ ì™„ë£Œ: ì´ {stats['saved_chunks']}ê°œ ì²­í¬ ì €ì¥ (ì†Œìš” ì‹œê°„: {save_elapsed:.1f}ì´ˆ)")
        
    except Exception as e:
        print(f"âŒ ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
        stats["errors"] += len(all_texts)
    
    return stats


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================


def vectorize_news(
    limit: Optional[int] = None,
    skip: int = 0,
    collection_name: Optional[str] = None
) -> Dict[str, int]:
    """
    MongoDBì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ ChromaDBì— ì €ì¥
    
    Args:
        limit: ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ (Noneì´ë©´ ëª¨ë‘)
        skip: ê±´ë„ˆë›¸ ê¸°ì‚¬ ìˆ˜
        collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
    
    Returns:
        í†µê³„ ì •ë³´
    """
    # collection_nameì´ Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if collection_name is None:
        collection_name = CHROMADB_COLLECTION_NAME
    
    print("=" * 70)
    print("ğŸ”¢ Investing.com ë‰´ìŠ¤ ë²¡í„°í™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    print("=" * 70)
    
    # 1. MongoDB ì—°ê²°
    print(f"\n1ï¸âƒ£ MongoDB ì—°ê²° ì¤‘: {MONGODB_HOST}:{MONGODB_PORT}/{MONGODB_DB}/{MONGODB_NEWS_COLLECTION}")
    mongo_client = get_mongodb_client()
    mongo_collection = get_mongodb_news_collection(mongo_client)
    
    # 2. MongoDBì—ì„œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
    query = {}  # ëª¨ë“  ê¸°ì‚¬
    if limit:
        articles = list(mongo_collection.find(query).skip(skip).limit(limit))
    else:
        articles = list(mongo_collection.find(query).skip(skip))
    print(f"   ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    if not articles:
        print("   ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        mongo_client.close()
        return {
            "total_articles": 0,
            "total_chunks": 0,
            "saved_chunks": 0,
            "errors": 0
        }
    
    # 3. BGE-M3 ëª¨ë¸ ì´ˆê¸°í™” (Redditì²˜ëŸ¼ FlagModel ì§ì ‘ ì‚¬ìš©)
    print(f"\n2ï¸âƒ£ BGE-M3 ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    try:
        if BGE_M3_MODEL_PATH and os.path.exists(BGE_M3_MODEL_PATH):
            embedder = FlagModel(BGE_M3_MODEL_PATH, use_fp16=True)
            print(f"   ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {BGE_M3_MODEL_PATH}")
        else:
            embedder = FlagModel(BGE_M3_MODEL_NAME, use_fp16=True)
            print(f"   Hugging Face ëª¨ë¸ ì‚¬ìš©: {BGE_M3_MODEL_NAME}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        mongo_client.close()
        return {
            "total_articles": len(articles),
            "total_chunks": 0,
            "saved_chunks": 0,
            "errors": 0
        }
    
    # 4. ChromaDB ì—°ê²°
    print(f"\n3ï¸âƒ£ ChromaDB ì—°ê²° ì¤‘: {CHROMADB_URL}:{CHROMADB_PORT}/{collection_name}")
    chroma_client = make_chroma_client()
    chroma_collection = get_or_create_chromadb_collection(chroma_client, collection_name)
    
    # 5. ë²¡í„°í™” ë° ì €ì¥
    print(f"\n4ï¸âƒ£ ë²¡í„°í™” ë° ì €ì¥ ì¤‘ ({len(articles)}ê°œ ê¸°ì‚¬)...")
    stats = save_to_chromadb(articles, chroma_collection, embedder)
    
    # 6. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
    print("=" * 70)
    print(f"ì²˜ë¦¬í•œ ê¸°ì‚¬: {stats['total_articles']}ê°œ")
    print(f"ìƒì„±ëœ ì²­í¬: {stats['total_chunks']}ê°œ")
    print(f"ì €ì¥ëœ ì²­í¬: {stats['saved_chunks']}ê°œ")
    print(f"ì˜¤ë¥˜ ë°œìƒ: {stats['errors']}ê°œ")
    print("=" * 70)
    
    # ì—°ê²° ì¢…ë£Œ
    mongo_client.close()
    
    return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì „ì²´ ê¸°ì‚¬ ë²¡í„°í™”
    stats = vectorize_news()
    
    print("\nâœ… ë²¡í„°í™” í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    import json
    print(json.dumps(stats, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()

