# -*- coding: utf-8 -*-

"""
reddit_stocks.json ë°ì´í„°ë¥¼ kssì™€ bge-m3ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°í™”í•˜ì—¬ ChromaDBì— ì €ì¥
- KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬ í›„, 400ì ì²­í¬ë¡œ ë¬¶ê¸° (100ì overlap)
- ê° ì²­í¬ì— ì œëª© í¬í•¨
- ë‰´ìŠ¤ ë°ì´í„°ì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ì €ì¥í•˜ë˜ ë³„ë„ ì»¬ë ‰ì…˜ ì‚¬ìš©
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import time
import json

# Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ê¶Œí•œ ë¬¸ì œ í•´ê²°)
if not os.getenv("HF_HOME"):
    os.environ["HF_HOME"] = "/opt/airflow/.cache/huggingface"
if not os.getenv("TRANSFORMERS_CACHE"):
    os.environ["TRANSFORMERS_CACHE"] = "/opt/airflow/.cache/huggingface"
if not os.getenv("HF_DATASETS_CACHE"):
    os.environ["HF_DATASETS_CACHE"] = "/opt/airflow/.cache/huggingface"

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
cache_dir = os.environ.get("HF_HOME", "/opt/airflow/.cache/huggingface")
os.makedirs(cache_dir, exist_ok=True)
try:
    os.chmod(cache_dir, 0o755)
except PermissionError:
    pass

# ChromaDB ì§ì ‘ ì—°ê²°
from chromadb import HttpClient
from chromadb.config import Settings

from dotenv import load_dotenv
import kss
from FlagEmbedding import FlagModel

# .env íŒŒì¼ ê²½ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • (Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ì‚¬ìš©)
# docker-composeì—ì„œ /opt/airflow/.envë¡œ ë§ˆìš´íŠ¸ë¨
# override=True: ê¸°ì¡´ í™˜ê²½ ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì˜ ê°’ìœ¼ë¡œ ë®ì–´ì”€
env_path = Path("/opt/airflow/.env")
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
else:
    # ì ˆëŒ€ ê²½ë¡œì—ì„œë„ ì‹œë„
    env_path_abs = Path("/opt/S13P31B205/ai-service/.env")
    if env_path_abs.exists():
        load_dotenv(dotenv_path=env_path_abs, override=True)
    else:
        # ê¸°ë³¸ ê²½ë¡œì—ì„œë„ ì‹œë„
        load_dotenv(override=True)

# ============================================================================
# í™˜ê²½ ë³€ìˆ˜
# ============================================================================

# ChromaDB ì„¤ì •
CHROMADB_URL = os.getenv("CHROMADB_URL", "3.34.50.3")
CHROMADB_PORT = int(os.getenv("CHROMADB_PORT", "9000"))
CHROMADB_COLLECTION_NAME = os.getenv("CHROMADB_REDDIT_COLLECTION_NAME", "reddit_stocks_bge_m3")

# BGE-M3 ëª¨ë¸ ì„¤ì •
BGE_M3_MODEL_NAME = "BAAI/bge-m3"
BGE_M3_MODEL_PATH = os.getenv("BGE_M3_MODEL_PATH", None)

# Reddit ë°ì´í„° íŒŒì¼ ê²½ë¡œ
REDDIT_STOCKS_JSON = os.getenv("REDDIT_STOCKS_JSON", "/opt/airflow/data/reddit_stocks.json")


# ============================================================================
# ChromaDB ì—°ê²°
# ============================================================================


def make_chroma_client():
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return HttpClient(
        host=CHROMADB_URL,
        port=CHROMADB_PORT,
        settings=Settings(anonymized_telemetry=False),
    )


# ============================================================================
# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================


def load_reddit_data(json_file: str) -> List[Dict]:
    """
    reddit_stocks.json íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
    
    Returns:
        Reddit í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not os.path.exists(json_file):
        print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {json_file}")
        return []
    
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # ë°ì´í„° êµ¬ì¡°: [{crawled_at, subreddits, posts: [...]}]
        all_posts = []
        for entry in data:
            if isinstance(entry, dict) and "posts" in entry:
                for post in entry["posts"]:
                    # Reddit í¬ìŠ¤íŠ¸ë¥¼ ë‰´ìŠ¤ì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ë³€í™˜
                    reddit_post = {
                        "title": post.get("title", ""),
                        "content": post.get("content", ""),
                        "date": post.get("ë‚ ì§œ", ""),
                        "url": post.get("url", ""),
                        "permalink": post.get("permalink", ""),
                        "subreddit": post.get("subreddit", ""),
                        "score": post.get("score", 0),
                        "num_comments": post.get("num_comments", 0),
                        "source": "reddit"  # ì¶œì²˜ êµ¬ë¶„
                    }
                    all_posts.append(reddit_post)
        
        print(f"âœ… Reddit ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_posts)}ê°œ í¬ìŠ¤íŠ¸")
        return all_posts
    
    except Exception as e:
        print(f"âŒ Reddit ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return []


def chunk_text(text: str, chunk_size: int = 400, overlap: int = 100) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë‰´ìŠ¤ì™€ ë™ì¼í•œ ë°©ì‹)
    - KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬ í›„, chunk_size ê¸¸ì´ë¡œ ë¬¶ê¸°
    - overlapë§Œí¼ ê²¹ì¹˜ê²Œ ì„¤ì •
    """
    if not text or not text.strip():
        return []
    
    # KSSë¡œ ë¬¸ì¥ ë¶„ë¦¬
    try:
        sentences = kss.split_sentences(text)
    except Exception as e:
        print(f"âš ï¸ ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}, ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬")
        sentences = [text]
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ì‹œ ê¸¸ì´ í™•ì¸
        test_chunk = current_chunk + (" " if current_chunk else "") + sentence
        
        if len(test_chunk) <= chunk_size:
            current_chunk = test_chunk
        else:
            # í˜„ì¬ ì²­í¬ ì €ì¥
            if current_chunk:
                chunks.append(current_chunk)
            
            # overlap ê³ ë ¤í•˜ì—¬ ìƒˆ ì²­í¬ ì‹œì‘
            if overlap > 0 and current_chunk:
                # ë§ˆì§€ë§‰ overlap ê¸¸ì´ë§Œí¼ ê°€ì ¸ì˜¤ê¸°
                overlap_text = current_chunk[-overlap:] if len(current_chunk) >= overlap else current_chunk
                current_chunk = overlap_text + " " + sentence
            else:
                current_chunk = sentence
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


def prepare_chunks_for_reddit(posts: List[Dict]) -> List[Dict]:
    """
    Reddit í¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•  ìˆ˜ ìˆëŠ” ì²­í¬ë¡œ ë³€í™˜
    - ì œëª© + ë³¸ë¬¸ì„ í•©ì³ì„œ ì²­í¬ ìƒì„±
    - ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    all_chunks = []
    
    for post_idx, post in enumerate(posts):
        title = post.get("title", "").strip()
        content = post.get("content", "").strip()
        
        # ì œëª©ê³¼ ë³¸ë¬¸ ê²°í•©
        if content:
            full_text = f"{title}\n\n{content}"
        else:
            full_text = title
        
        if not full_text.strip():
            continue
        
        # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
        text_chunks = chunk_text(full_text, chunk_size=400, overlap=100)
        
        # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for chunk_idx, chunk in enumerate(text_chunks):
            chunk_data = {
                "text": chunk,
                "title": title[:200],  # ChromaDB ë©”íƒ€ë°ì´í„° ê¸¸ì´ ì œí•œ
                "date": post.get("date", ""),
                "url": post.get("url", ""),
                "permalink": post.get("permalink", ""),
                "subreddit": post.get("subreddit", ""),
                "score": post.get("score", 0),
                "num_comments": post.get("num_comments", 0),
                "source": "reddit",
                "chunk_index": chunk_idx,
                "total_chunks": len(text_chunks),
                "post_index": post_idx
            }
            all_chunks.append(chunk_data)
    
    return all_chunks


# ============================================================================
# ChromaDB ì €ì¥
# ============================================================================


def get_or_create_chromadb_collection(client, collection_name: str):
    """ChromaDB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
    try:
        collection = client.get_collection(collection_name)
        print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {collection_name}")
        return collection
    except Exception:
        # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
        collection = client.create_collection(
            name=collection_name,
            metadata={"description": "Reddit stocks posts vectorized with bge-m3"}
        )
        print(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
        return collection


def save_to_chromadb(chunks: List[Dict], collection, embedder):
    """
    ê°€ê³µëœ Reddit í¬ìŠ¤íŠ¸ ì²­í¬ë¥¼ ChromaDBì— ë²¡í„°í™”í•˜ì—¬ ì €ì¥
    
    Args:
        chunks: ì²­í¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        collection: ChromaDB ì»¬ë ‰ì…˜
        embedder: BGE-M3 ì„ë² ë”© ëª¨ë¸
    """
    if not chunks:
        print("âš ï¸ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"saved_chunks": 0, "skipped": 0}
    
    stats = {"saved_chunks": 0, "skipped": 0}
    
    # ê¸°ì¡´ ë¬¸ì„œ ID í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
    existing_ids = set()
    try:
        existing_docs = collection.get()
        if existing_docs and existing_docs.get("ids"):
            existing_ids = set(existing_docs["ids"])
            print(f"   ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {len(existing_ids)}ê°œ")
    except Exception:
        pass
    
    import urllib.parse
    
    # ì¤‘ë³µ ì œê±°: ë²¡í„°í™” ì „ì— ì¤‘ë³µ ì²´í¬í•˜ì—¬ í•„í„°ë§
    new_chunks = []
    
    for chunk_idx, chunk in enumerate(chunks):
        # ê³ ìœ  ID ìƒì„±
        permalink = chunk.get('permalink', '')
        chunk_index = chunk.get('chunk_index', 0)
        doc_id = f"reddit_{permalink}_{chunk_index}"
        doc_id = urllib.parse.quote(doc_id, safe='')
        
        # ì¤‘ë³µ ì²´í¬ (ë²¡í„°í™” ì „ì— ìˆ˜í–‰)
        if doc_id in existing_ids:
            stats["skipped"] += 1
            continue
        
        new_chunks.append(chunk)
    
    if stats["skipped"] > 0:
        print(f"   âš ï¸ ì¤‘ë³µ ê±´ë„ˆëœ€: {stats['skipped']}ê°œ ì²­í¬ (ë²¡í„°í™” ì „ í•„í„°ë§)")
    
    if not new_chunks:
        print("   âš ï¸ ì €ì¥í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return stats
    
    # ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°ëœ ì²­í¬ë§Œ)
    all_texts = [chunk["text"] for chunk in new_chunks]
    
    # ë²¡í„°í™” (ì¤‘ë³µ ì œê±°ëœ ì²­í¬ë§Œ)
    vectorize_batch_size = 64
    print(f"   ë²¡í„°í™” ì¤‘... (ì´ {len(all_texts)}ê°œ ì²­í¬, ë°°ì¹˜ í¬ê¸°: {vectorize_batch_size})")
    vectorize_start = time.time()
    try:
        embeddings = embedder.encode(all_texts, batch_size=vectorize_batch_size)
        vectorize_elapsed = time.time() - vectorize_start
        print(f"   âœ… ë²¡í„°í™” ì™„ë£Œ (ì†Œìš” ì‹œê°„: {vectorize_elapsed:.1f}ì´ˆ)")
    except Exception as e:
        print(f"   âŒ ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")
        return stats
    
    # ChromaDBì— ì €ì¥ (ë°°ì¹˜ ë‹¨ìœ„)
    batch_size = 100
    total_batches = (len(new_chunks) + batch_size - 1) // batch_size
    
    save_start = time.time()
    
    # ëª¨ë“  ì²­í¬ì— ëŒ€í•´ ID, ë©”íƒ€ë°ì´í„°, ë¬¸ì„œ ì¤€ë¹„
    all_ids = []
    all_metadatas = []
    all_documents = []
    all_embeddings_list = []
    
    for chunk_idx, chunk in enumerate(new_chunks):
        # ê³ ìœ  ID ìƒì„±
        permalink = chunk.get('permalink', '')
        chunk_index = chunk.get('chunk_index', 0)
        doc_id = f"reddit_{permalink}_{chunk_index}"
        doc_id = urllib.parse.quote(doc_id, safe='')
        
        all_ids.append(doc_id)
        all_documents.append(chunk["text"])
        all_embeddings_list.append(embeddings[chunk_idx])
        
        # ë©”íƒ€ë°ì´í„° (ChromaDB ì œí•œ: ë¬¸ìì—´ ê°’ë§Œ ê°€ëŠ¥)
        metadata = {
            "title": str(chunk.get("title", ""))[:200],
            "date": str(chunk.get("date", ""))[:100],
            "url": str(chunk.get("url", ""))[:500],
            "permalink": str(permalink)[:500],
            "subreddit": str(chunk.get("subreddit", ""))[:100],
            "score": str(chunk.get("score", 0)),
            "num_comments": str(chunk.get("num_comments", 0)),
            "source": "reddit",
            "chunk_index": str(chunk_index),
            "total_chunks": str(chunk.get("total_chunks", 1))
        }
        all_metadatas.append(metadata)
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì €ì¥
    for batch_idx in range(0, len(all_ids), batch_size):
        batch_num = (batch_idx // batch_size) + 1
        end_idx = min(batch_idx + batch_size, len(all_ids))
        
        batch_ids = all_ids[batch_idx:end_idx]
        batch_documents = all_documents[batch_idx:end_idx]
        batch_metadatas = all_metadatas[batch_idx:end_idx]
        batch_embeddings = all_embeddings_list[batch_idx:end_idx]
        
        # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if hasattr(batch_embeddings[0], 'tolist'):
            batch_embeddings = [emb.tolist() for emb in batch_embeddings]
        elif hasattr(batch_embeddings, 'tolist'):
            batch_embeddings = batch_embeddings.tolist()
        
        try:
            collection.add(
                ids=batch_ids,
                embeddings=batch_embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas
            )
            stats["saved_chunks"] += len(batch_ids)
            print(f"   ë°°ì¹˜ {batch_num}/{total_batches} ì €ì¥ ì™„ë£Œ: {len(batch_ids)}ê°œ ì²­í¬")
        except Exception as e:
            print(f"   âš ï¸ ë°°ì¹˜ {batch_num} ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    save_elapsed = time.time() - save_start
    print(f"   âœ… ChromaDB ì €ì¥ ì™„ë£Œ: ì´ {stats['saved_chunks']}ê°œ ì²­í¬ ì €ì¥ (ì†Œìš” ì‹œê°„: {save_elapsed:.1f}ì´ˆ)")
    if stats["skipped"] > 0:
        print(f"   âš ï¸ ì¤‘ë³µ ê±´ë„ˆëœ€: {stats['skipped']}ê°œ ì²­í¬")
    
    return stats


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================


def vectorize_reddit_stocks(
    json_file: str = None,
    collection_name: str = None
):
    """
    reddit_stocks.json ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ ChromaDBì— ì €ì¥
    
    Args:
        json_file: Reddit JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
        collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
    """
    json_file = json_file or REDDIT_STOCKS_JSON
    collection_name = collection_name or CHROMADB_COLLECTION_NAME
    
    print("=" * 70)
    print("ğŸ”„ Reddit Stocks ë°ì´í„° ë²¡í„°í™” ì‹œì‘")
    print("=" * 70)
    
    # 1. Reddit ë°ì´í„° ë¡œë“œ
    print(f"\n1ï¸âƒ£ Reddit ë°ì´í„° ë¡œë“œ ì¤‘: {json_file}")
    posts = load_reddit_data(json_file)
    
    if not posts:
        print("âš ï¸ Reddit ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return {"status": "no_data", "saved_chunks": 0}
    
    print(f"   ë¡œë“œëœ í¬ìŠ¤íŠ¸: {len(posts)}ê°œ")
    
    # 2. BGE-M3 ëª¨ë¸ ë¡œë“œ
    print(f"\n2ï¸âƒ£ BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘: {BGE_M3_MODEL_NAME}")
    model_start = time.time()
    try:
        if BGE_M3_MODEL_PATH and os.path.exists(BGE_M3_MODEL_PATH):
            embedder = FlagModel(BGE_M3_MODEL_PATH, use_fp16=True)
            print(f"   ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {BGE_M3_MODEL_PATH}")
        else:
            embedder = FlagModel(BGE_M3_MODEL_NAME, use_fp16=True)
            print(f"   Hugging Face ëª¨ë¸ ì‚¬ìš©: {BGE_M3_MODEL_NAME}")
        model_elapsed = time.time() - model_start
        print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {model_elapsed:.1f}ì´ˆ)")
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"status": "model_load_failed", "saved_chunks": 0}
    
    # 3. í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±
    print(f"\n3ï¸âƒ£ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì¤‘...")
    chunks = prepare_chunks_for_reddit(posts)
    print(f"   ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")
    
    # 4. ChromaDB ì—°ê²°
    print(f"\n4ï¸âƒ£ ChromaDB ì—°ê²° ì¤‘: {CHROMADB_URL}:{CHROMADB_PORT}/{collection_name}")
    chroma_client = make_chroma_client()
    chroma_collection = get_or_create_chromadb_collection(chroma_client, collection_name)
    
    # 5. ChromaDB ì €ì¥
    print(f"\n5ï¸âƒ£ ChromaDB ì €ì¥ ì¤‘...")
    stats = save_to_chromadb(chunks, chroma_collection, embedder)
    
    print("\n" + "=" * 70)
    print("âœ… Reddit Stocks ë²¡í„°í™” ì™„ë£Œ!")
    print("=" * 70)
    print(f"ì €ì¥ëœ ì²­í¬: {stats['saved_chunks']}ê°œ")
    if stats.get("skipped", 0) > 0:
        print(f"ì¤‘ë³µ ê±´ë„ˆëœ€: {stats['skipped']}ê°œ")
    
    return {"status": "success", **stats}


if __name__ == "__main__":
    stats = vectorize_reddit_stocks()
    print(f"\nìµœì¢… ê²°ê³¼: {json.dumps(stats, indent=2, ensure_ascii=False)}")

