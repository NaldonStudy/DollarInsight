"""
Reddit ì£¼ì‹ ê²Œì‹œê¸€ í¬ë¡¤ë§ Airflow DAG
2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰í•˜ì—¬ ì¸ê¸° ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•˜ê³  JSON íŒŒì¼ì— ëˆ„ì  ì €ì¥
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

# Airflow utils ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
airflow_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
utils_dir = os.path.join(airflow_dir, "utils")
if utils_dir not in sys.path:
    sys.path.insert(0, utils_dir)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
project_root = airflow_dir  # /opt/airflow

# Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
# from crawl_reddit_stocks import RedditPostsCrawler


# ê¸°ë³¸ ì¸ì ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=10),
    "start_date": datetime(2024, 1, 1),
}

# DAG ì •ì˜
dag = DAG(
    "reddit_stocks_crawler",
    default_args=default_args,
    description="Reddit ì£¼ì‹ ê²Œì‹œê¸€ í¬ë¡¤ë§ - 2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰",
    schedule="0 */2 * * *",  # 2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (ë§¤ ì§ìˆ˜ ì‹œ 0ë¶„) - schedule_interval ëŒ€ì‹  schedule ì‚¬ìš©
    catchup=False,
    max_active_runs=1,  # ë™ì‹œì— ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” ê°™ì€ DAG ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ (1ê°œë§Œ í—ˆìš©)
    max_active_tasks=1,  # ë™ì‹œì— ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” Task ìˆ˜
    tags=["reddit", "stocks", "crawling", "social"],
)


def crawl_reddit_stocks(**context):
    """Reddit ì£¼ì‹ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì‹¤í–‰"""
    # Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
    from crawl_reddit_stocks import RedditPostsCrawler
    import os
    from datetime import datetime

    # ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
    data_dir = os.path.join(project_root, "data")
    os.makedirs(data_dir, exist_ok=True)
    json_file = os.path.join(data_dir, "reddit_stocks.json")

    print(f"ğŸ”„ í¬ë¡¤ë§ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {json_file}")

    try:
        # í¬ë¡¤ëŸ¬ ìƒì„± (ìµœì†Œ score 100ì  ì´ìƒ í•„í„°ë§)
        # Reddit ê³µê°œ API ì‚¬ìš© (ì¸ì¦ ë¶ˆí•„ìš”)
        crawler = RedditPostsCrawler(min_score=100)

        # í¬ë¡¤ë§ ì‹¤í–‰ (ê° ì„œë¸Œë ˆë”§ë‹¹ 25ê°œ ê²Œì‹œê¸€)
        results = crawler.crawl(limit_per_subreddit=25)

        if results and results.get("posts"):
            # ê¸°ì¡´ JSON íŒŒì¼ì— ëˆ„ì  ì €ì¥ (permalink ê¸°ì¤€ ì¤‘ë³µ ì œê±°)
            new_count = crawler.append_to_json(results, filename=json_file)

            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ:")
            print(f"   - ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(results['posts'])}ê°œ")
            print(f"   - ì‹ ê·œ ì¶”ê°€: {new_count}ê°œ")
            print(f"   - ìµœì†Œ score: {results.get('min_score', 5)}ì  ì´ìƒ")

            # Airflow XComì— ê²°ê³¼ ì €ì¥
            context["ti"].xcom_push(key="total_posts", value=len(results["posts"]))
            context["ti"].xcom_push(key="new_posts", value=new_count)

            return {
                "status": "success",
                "total_posts": len(results["posts"]),
                "new_posts": new_count,
            }
        else:
            print("âš ï¸ í¬ë¡¤ë§ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "no_posts",
                "total_posts": 0,
                "new_posts": 0,
            }

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        raise


# í¬ë¡¤ë§ ì‘ì—… ì •ì˜
crawl_task = PythonOperator(
    task_id="crawl_reddit_stocks",
    python_callable=crawl_reddit_stocks,
    dag=dag,
)

# ì‘ì—… ì‹¤í–‰
crawl_task
