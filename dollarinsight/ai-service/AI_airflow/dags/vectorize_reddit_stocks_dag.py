"""
Reddit Stocks ë°ì´í„° ë²¡í„°í™” Airflow DAG
reddit_stocks.json ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ ChromaDBì— ì €ì¥
í¬ë¡¤ë§ í›„ ì‹¤í–‰ (ë§¤ì¼ ì˜¤ì „ 3ì‹œ 30ë¶„)
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

# Airflow utils ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
airflow_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
utils_dir = os.path.join(airflow_dir, "utils")
if utils_dir not in sys.path:
    sys.path.insert(0, utils_dir)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
project_root = airflow_dir  # /opt/airflow

# ëª¨ë“ˆ ë ˆë²¨ import ì œê±° (íŒŒì‹± ì‹œ CPU ê³¼ë¶€í•˜ ë°©ì§€)
# ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬(FlagEmbedding, kss, chromadb) ë¡œë”©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´
# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ë„ë¡ ë³€ê²½
# from vectorize_reddit_stocks import vectorize_reddit_stocks

# ê¸°ë³¸ ì¸ì ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=10),
    "start_date": datetime(2024, 1, 1),
}

# DAG ì •ì˜
dag = DAG(
    "vectorize_reddit_stocks",
    default_args=default_args,
    description="Reddit Stocks ë°ì´í„° ë²¡í„°í™” - reddit_stocks.jsonì„ ChromaDBì— ì €ì¥",
    schedule="30 3 * * *",  # ë§¤ì¼ ì˜¤ì „ 3ì‹œ 30ë¶„ ì‹¤í–‰ (ë‰´ìŠ¤ ë²¡í„°í™”ì™€ ë™ì‹œ)
    catchup=False,
    max_active_runs=1,
    max_active_tasks=1,
    tags=["reddit", "vectorization", "chromadb"],
)


def vectorize_reddit_task(**context):
    """Reddit Stocks ë°ì´í„° ë²¡í„°í™” ì‹¤í–‰"""
    import os
    from datetime import datetime

    # ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ importë¥¼ í•¨ìˆ˜ ë‚´ë¶€ë¡œ ì´ë™ (íŒŒì‹± ì‹œ CPU ê³¼ë¶€í•˜ ë°©ì§€)
    from vectorize_reddit_stocks import vectorize_reddit_stocks

    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_dir = os.path.join(project_root, "data")
    json_file = os.path.join(data_dir, "reddit_stocks.json")

    print(
        f"ğŸ”„ Reddit Stocks ë²¡í„°í™” ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )
    print(f"ğŸ“ ë°ì´í„° íŒŒì¼: {json_file}")

    try:
        # ë²¡í„°í™” ì‹¤í–‰
        stats = vectorize_reddit_stocks(json_file=json_file)

        # Airflow XComì— ê²°ê³¼ ì €ì¥
        context["ti"].xcom_push(key="saved_chunks", value=stats.get("saved_chunks", 0))
        context["ti"].xcom_push(key="skipped", value=stats.get("skipped", 0))
        context["ti"].xcom_push(key="status", value=stats.get("status", "unknown"))

        return {
            "status": stats.get("status", "unknown"),
            "saved_chunks": stats.get("saved_chunks", 0),
            "skipped": stats.get("skipped", 0),
        }

    except Exception as e:
        print(f"âŒ Reddit Stocks ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        raise


# ë²¡í„°í™” ì‘ì—… ì •ì˜
vectorize_task = PythonOperator(
    task_id="vectorize_reddit_stocks",
    python_callable=vectorize_reddit_task,
    dag=dag,
)

# ì‘ì—… ì‹¤í–‰
vectorize_task
