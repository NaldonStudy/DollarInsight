"""
Investing.com ë‰´ìŠ¤ í¬ë¡¤ë§ Airflow DAG
30ë¶„ë§ˆë‹¤ ì‹¤í–‰í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  JSON íŒŒì¼ì— ëˆ„ì  ì €ì¥
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

# Airflow utils ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
# dagsì™€ utilsê°€ ëª¨ë‘ airflow/ ì•ˆì— ìˆìœ¼ë¯€ë¡œ ê°™ì€ ë ˆë²¨
airflow_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
utils_dir = os.path.join(airflow_dir, "utils")
if utils_dir not in sys.path:
    sys.path.insert(0, utils_dir)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (ë°ì´í„° ì €ì¥ìš©)
# Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ /opt/airflowê°€ ë£¨íŠ¸ì´ë¯€ë¡œ airflow_dir ì‚¬ìš©
project_root = airflow_dir  # /opt/airflow

# Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
# from crawl_investing_news import InvestingNewsCrawler


# ê¸°ë³¸ ì¸ì ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "start_date": datetime(2024, 1, 1),
}

# DAG ì •ì˜
dag = DAG(
    "investing_news_crawler",
    default_args=default_args,
    description="Investing.com ë‰´ìŠ¤ í¬ë¡¤ë§ - 30ë¶„ë§ˆë‹¤ ì‹¤í–‰",
    schedule="*/30 * * * *",  # 30ë¶„ë§ˆë‹¤ ì‹¤í–‰ - schedule_interval ëŒ€ì‹  schedule ì‚¬ìš©
    catchup=False,
    max_active_runs=1,  # ë™ì‹œì— ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” ê°™ì€ DAG ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ (1ê°œë§Œ í—ˆìš©)
    max_active_tasks=1,  # ë™ì‹œì— ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” Task ìˆ˜
    tags=["news", "crawling", "investing"],
)


def crawl_investing_news(**context):
    """Investing.com ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰"""
    # Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
    from crawl_investing_news import InvestingNewsCrawler
    import os
    from datetime import datetime

    # ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
    # Docker ì»¨í…Œì´ë„ˆ: /opt/airflow/data â†’ í˜¸ìŠ¤íŠ¸: AI_airflow/data
    data_dir = os.path.join(project_root, "data")
    os.makedirs(data_dir, exist_ok=True)
    json_file = os.path.join(data_dir, "investing_news.json")

    print(f"ğŸ”„ í¬ë¡¤ë§ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {json_file} (í˜¸ìŠ¤íŠ¸: AI_airflow/data/investing_news.json)")

    try:
        # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = InvestingNewsCrawler()
        results = crawler.crawl(max_articles=10)

        if results:
            # ê¸°ì¡´ JSON íŒŒì¼ì— ëˆ„ì  ì €ì¥
            new_count = crawler.append_to_json(results, filename=json_file)
            print(
                f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {new_count}ê°œ ìƒˆ ê¸°ì‚¬ ì¶”ê°€, ì´ {len(results)}ê°œ ì²˜ë¦¬"
            )

            # Airflow XComì— ê²°ê³¼ ì €ì¥
            context["ti"].xcom_push(key="new_articles", value=new_count)
            context["ti"].xcom_push(key="total_processed", value=len(results))

            return {
                "status": "success",
                "new_articles": new_count,
                "total_processed": len(results),
            }
        else:
            print("âš ï¸ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "no_articles",
                "new_articles": 0,
                "total_processed": 0,
            }

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        raise


# í¬ë¡¤ë§ ì‘ì—… ì •ì˜
crawl_task = PythonOperator(
    task_id="crawl_investing_news",
    python_callable=crawl_investing_news,
    dag=dag,
)

# ì‘ì—… ì‹¤í–‰ ìˆœì„œ ì„¤ì • (í˜„ì¬ëŠ” ë‹¨ì¼ ì‘ì—…ì´ì§€ë§Œ í–¥í›„ í™•ì¥ ê°€ëŠ¥)
crawl_task
