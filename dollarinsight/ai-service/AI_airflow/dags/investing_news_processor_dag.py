"""
Investing.com ë‰´ìŠ¤ ê°€ê³µ ë° MongoDB ì €ì¥ DAG
í•˜ë£¨ì— í•œ ë²ˆ ì‹¤í–‰í•˜ì—¬ investing_news.jsonì„ ê°€ê³µí•˜ì—¬ MongoDBì— ì €ì¥
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os
import pytz

# Airflow utils ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
airflow_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
utils_dir = os.path.join(airflow_dir, "utils")
if utils_dir not in sys.path:
    sys.path.insert(0, utils_dir)

# Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
# from process_investing_news import (
#     process_and_save,
#     get_mongodb_client,
#     get_mongodb_news_collection,
#     get_mongodb_persona_collection,
# )

# ê¸°ë³¸ ì¸ì ì„¤ì •
# í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
kst = pytz.timezone("Asia/Seoul")
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=10),
    "start_date": datetime(2024, 1, 1, tzinfo=kst),  # í•œêµ­ ì‹œê°„ëŒ€ ì ìš©
}

# DAG ì •ì˜
dag = DAG(
    "investing_news_processor",
    default_args=default_args,
    description="Investing.com ë‰´ìŠ¤ ê°€ê³µ ë° MongoDB ì €ì¥ - í•˜ë£¨ì— í•œ ë²ˆ ì‹¤í–‰",
    schedule="0 3 * * *",  # ë§¤ì¼ ì˜¤ì „ 3ì‹œ ì‹¤í–‰ (í¬ë¡¤ë§ í›„, í•œêµ­ ì‹œê°„ ê¸°ì¤€)
    catchup=False,
    max_active_runs=1,
    max_active_tasks=1,
    tags=["news", "processing", "mongodb"],
)


def process_investing_news_task(**context):
    """Investing.com ë‰´ìŠ¤ ê°€ê³µ ë° MongoDB ì €ì¥ ì‹¤í–‰"""
    # Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
    from process_investing_news import (
        process_and_save,
        get_mongodb_client,
        get_mongodb_news_collection,
        get_mongodb_persona_collection,
    )
    import os
    from datetime import datetime
    
    # JSON íŒŒì¼ ê²½ë¡œ
    data_dir = os.path.join(airflow_dir, "data")
    json_path = os.path.join(data_dir, "investing_news.json")
    
    print(f"ğŸ”„ ë‰´ìŠ¤ ê°€ê³µ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {json_path}")
    
    if not os.path.exists(json_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return {
            "status": "error",
            "message": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}",
        }
    
    try:
        # MongoDB ì—°ê²°
        client = get_mongodb_client()
        news_collection = get_mongodb_news_collection(client)
        persona_collection = get_mongodb_persona_collection(client)
        
        # ë‰´ìŠ¤ ê°€ê³µ ë° ì €ì¥
        stats = process_and_save(
            json_path=json_path,
            news_collection=news_collection,
            persona_collection=persona_collection
        )
        
        client.close()
        
        # Airflow XComì— ê²°ê³¼ ì €ì¥
        context["ti"].xcom_push(key="total_articles", value=stats["total"])
        context["ti"].xcom_push(key="new_articles", value=stats["new"])
        context["ti"].xcom_push(key="news_inserted", value=stats["news_inserted"])
        context["ti"].xcom_push(key="persona_inserted", value=stats["persona_inserted"])
        
        return {
            "status": "success",
            "total_articles": stats["total"],
            "new_articles": stats["new"],
            "news_inserted": stats["news_inserted"],
            "persona_inserted": stats["persona_inserted"],
        }
        
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ê°€ê³µ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


# ë‰´ìŠ¤ ê°€ê³µ ì‘ì—… ì •ì˜
process_task = PythonOperator(
    task_id="process_investing_news",
    python_callable=process_investing_news_task,
    dag=dag,
)

# ì‘ì—… ì‹¤í–‰
process_task

