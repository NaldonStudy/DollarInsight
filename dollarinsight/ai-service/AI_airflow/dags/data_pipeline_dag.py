"""
Data pipeline Airflow DAG

Raw data ìˆ˜ì§‘ â†’ Metrics & Scores ê³„ì‚° ìˆœì„œë¡œ ì‹¤í–‰
ë§¤ì¼ ìž¥ ë§ˆê° í›„ ì‹¤í–‰í•˜ì—¬ ì „ë‚  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë©”íŠ¸ë¦­ì„ ê³„ì‚°
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os
import pytz

# Airflow utils ê²½ë¡œë¥¼ Python ê²½ë¡œì— ì¶”ê°€
airflow_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
utils_dir = os.path.join(airflow_dir, "utils")
if utils_dir not in sys.path:
    sys.path.insert(0, utils_dir)

# Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
# from raw_data_pipeline import run_pipeline as run_raw_data_pipeline
# from metrics_scores_pipeline import run_pipeline as run_metrics_scores_pipeline


# ê¸°ë³¸ ì¸ìž ì„¤ì •
# í•œêµ­ ì‹œê°„(KST, UTC+9) ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
kst = pytz.timezone("Asia/Seoul")
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=30),
    "start_date": datetime(2024, 1, 1, tzinfo=kst),  # í•œêµ­ ì‹œê°„ëŒ€ ì ìš©
}

# DAG ì •ì˜
# scheduleì— cron í‘œí˜„ì‹ ì‚¬ìš© (UTC ê¸°ì¤€ìœ¼ë¡œ ì‹¤í–‰)
dag = DAG(
    "data_pipeline",
    default_args=default_args,
    description="Data pipeline: Raw data collection â†’ Metrics & Scores calculation",
    schedule="10 6 * * *",  # ë§¤ì¼ í•œêµ­ì‹œê°„ ê¸°ì¤€ ì˜¤ì „ 6ì‹œ 10ë¶„ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    max_active_tasks=2,  # ë‘ ìž‘ì—…ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ 2ë¡œ ì„¤ì •
    tags=["data", "collection", "metrics", "scores", "stocks"],
)


def run_raw_data(**context):
    """Raw data pipeline ì‹¤í–‰"""
    # Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
    from raw_data_pipeline import run_pipeline as run_raw_data_pipeline
    from datetime import datetime

    execution_date = context.get("execution_date") or context.get("data_interval_start")

    print(f"ðŸ”„ Raw data pipeline ì‹œìž‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ðŸ“… Execution date: {execution_date}")

    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (execution_date ì „ë‹¬)
        run_raw_data_pipeline(execution_date=execution_date)

        print("âœ… Raw data pipeline ì™„ë£Œ")

        return {
            "status": "success",
            "execution_date": str(execution_date),
        }

    except Exception as e:
        print(f"âŒ Raw data pipeline ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        raise


def run_metrics_scores(**context):
    """Metrics & scores pipeline ì‹¤í–‰"""
    # Lazy import: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ importí•˜ì—¬ DAG íŒŒì‹± ì‹œ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
    from metrics_scores_pipeline import run_pipeline as run_metrics_scores_pipeline
    from datetime import datetime

    execution_date = context.get("execution_date") or context.get("data_interval_start")

    print(
        f"ðŸ”„ Metrics & scores pipeline ì‹œìž‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )
    print(f"ðŸ“… Execution date: {execution_date}")

    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (execution_date ì „ë‹¬)
        run_metrics_scores_pipeline(execution_date=execution_date)

        print("âœ… Metrics & scores pipeline ì™„ë£Œ")

        return {
            "status": "success",
            "execution_date": str(execution_date),
        }

    except Exception as e:
        print(f"âŒ Metrics & scores pipeline ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        raise


# ìž‘ì—… ì •ì˜
raw_data_task = PythonOperator(
    task_id="run_raw_data_pipeline",
    python_callable=run_raw_data,
    dag=dag,
)

metrics_scores_task = PythonOperator(
    task_id="run_metrics_scores_pipeline",
    python_callable=run_metrics_scores,
    dag=dag,
)

# ìž‘ì—… ìˆœì„œ ì„¤ì •: raw_data_task ì™„ë£Œ í›„ metrics_scores_task ì‹¤í–‰
raw_data_task >> metrics_scores_task
