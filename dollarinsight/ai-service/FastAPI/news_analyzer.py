"""
ë‰´ìŠ¤ ë¶„ì„ ëª¨ë“ˆ
- ë‰´ìŠ¤ ìš”ì•½
- í˜ë¥´ì†Œë‚˜ 5ëª… ë¶„ì„
- ì˜í–¥ ë¯¸ì¹  ê¸°ì—… ëª©ë¡ ì¶”ì¶œ
"""

import os
import json
from typing import Dict, List
import openai

from prompts import AGENT_DESCRIPTIONS, AGENT_DATABASES
from database import (
    load_agent_collections,
    search_postgres,
)
from search import keyword_search_bm25, semantic_search_vector

# ì¶”ì  ëŒ€ìƒ ê¸°ì—…/ETF ëª©ë¡ (LLMì—ê²Œ ì œê³µí•˜ì—¬ ì •í™•í•œ ë§¤ì¹­)
TRACKED_COMPANIES = [
    # ê¸°ìˆ  ê¸°ì—… (12ê°œ)
    "ì• í”Œ",
    "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "ì•„ë§ˆì¡´",
    "ë©”íƒ€",
    "ì—”ë¹„ë””ì•„",
    "AMD",
    "ì¸í…”",
    "TSMC",
    "ASML",
    "ì–´ë„ë¹„",
    "ì˜¤ë¼í´",
    # ì»¤ë¨¸ìŠ¤ (2ê°œ)
    "ì¿ íŒ¡",
    "ì•Œë¦¬ë°”ë°”",
    # ìë™ì°¨ (1ê°œ)
    "í…ŒìŠ¬ë¼",
    # í•­ê³µ (2ê°œ)
    "ë³´ì‰",
    "ë¸íƒ€í•­ê³µ",
    # ëª¨ë¹Œë¦¬í‹° (1ê°œ)
    "ìš°ë²„",
    # ì‚°ì—…/ë¬¼ë¥˜ (1ê°œ)
    "í˜ë±ìŠ¤",
    # ë¦¬í…Œì¼ (2ê°œ)
    "ì›”ë§ˆíŠ¸",
    "ì½”ìŠ¤íŠ¸ì½”",
    # ê¸ˆìœµ (3ê°œ)
    "JPëª¨ê±´",
    "BOA",
    "ê³¨ë“œë§Œì‚­ìŠ¤",
    # ê²°ì œ (3ê°œ)
    "ë¹„ì",
    "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "í˜ì´íŒ”",
    # ë³´í—˜ (1ê°œ)
    "AIG",
    # ì†Œë¹„ì¬ (5ê°œ)
    "ì½”ì¹´ì½œë¼",
    "í©ì‹œ",
    "ë§¥ë„ë‚ ë“œ",
    "ìŠ¤íƒ€ë²…ìŠ¤",
    "ë‚˜ì´í‚¤",
    # ë¯¸ë””ì–´/ì—”í„° (3ê°œ)
    "ë„·í”Œë¦­ìŠ¤",
    "ë””ì¦ˆë‹ˆ",
    "ì†Œë‹ˆ",
    # ETF (14ê°œ)
    "VOO",
    "SPY",
    "VTI",
    "QQQ",
    "QQQM",
    "TQQQ",
    "SCHD",
    "SOXX",
    "SMH",
    "ITA",
    "XLF",
    "XLY",
    "XLP",
    "ICLN",
]

# ê¸°ì—…ëª… ë§¤í•‘ (ì˜ì–´/ë‹¤ì–‘í•œ í‘œê¸° -> í•œê¸€)
COMPANY_NAME_MAPPING = {
    # ê¸°ìˆ  ê¸°ì—…
    "apple": "ì• í”Œ",
    "aapl": "ì• í”Œ",
    "microsoft": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "msft": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "google": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "alphabet": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "googl": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "goog": "êµ¬ê¸€(ì•ŒíŒŒë²³)",
    "amazon": "ì•„ë§ˆì¡´",
    "amzn": "ì•„ë§ˆì¡´",
    "meta": "ë©”íƒ€",
    "facebook": "ë©”íƒ€",
    "fb": "ë©”íƒ€",
    "nvidia": "ì—”ë¹„ë””ì•„",
    "nvda": "ì—”ë¹„ë””ì•„",
    "amd": "AMD",
    "intel": "ì¸í…”",
    "intc": "ì¸í…”",
    "tsmc": "TSMC",
    "asml": "ASML",
    "adobe": "ì–´ë„ë¹„",
    "adbe": "ì–´ë„ë¹„",
    "oracle": "ì˜¤ë¼í´",
    "orcl": "ì˜¤ë¼í´",
    # ì»¤ë¨¸ìŠ¤
    "coupang": "ì¿ íŒ¡",
    "cpng": "ì¿ íŒ¡",
    "alibaba": "ì•Œë¦¬ë°”ë°”",
    "baba": "ì•Œë¦¬ë°”ë°”",
    # ìë™ì°¨
    "tesla": "í…ŒìŠ¬ë¼",
    "tsla": "í…ŒìŠ¬ë¼",
    # í•­ê³µ
    "boeing": "ë³´ì‰",
    "ba": "ë³´ì‰",
    "delta": "ë¸íƒ€í•­ê³µ",
    "dal": "ë¸íƒ€í•­ê³µ",
    # ëª¨ë¹Œë¦¬í‹°
    "uber": "ìš°ë²„",
    "uber": "ìš°ë²„",
    # ì‚°ì—…/ë¬¼ë¥˜
    "fedex": "í˜ë±ìŠ¤",
    "fdx": "í˜ë±ìŠ¤",
    # ë¦¬í…Œì¼
    "walmart": "ì›”ë§ˆíŠ¸",
    "wmt": "ì›”ë§ˆíŠ¸",
    "costco": "ì½”ìŠ¤íŠ¸ì½”",
    "cost": "ì½”ìŠ¤íŠ¸ì½”",
    # ê¸ˆìœµ
    "jpmorgan": "JPëª¨ê±´",
    "jpm": "JPëª¨ê±´",
    "jp morgan": "JPëª¨ê±´",
    "bank of america": "BOA",
    "boa": "BOA",
    "bac": "BOA",
    "goldman sachs": "ê³¨ë“œë§Œì‚­ìŠ¤",
    "gs": "ê³¨ë“œë§Œì‚­ìŠ¤",
    # ê²°ì œ
    "visa": "ë¹„ì",
    "v": "ë¹„ì",
    "mastercard": "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "ma": "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "paypal": "í˜ì´íŒ”",
    "pypl": "í˜ì´íŒ”",
    # ë³´í—˜
    "aig": "AIG",
    # ì†Œë¹„ì¬
    "coca-cola": "ì½”ì¹´ì½œë¼",
    "coca cola": "ì½”ì¹´ì½œë¼",
    "ko": "ì½”ì¹´ì½œë¼",
    "pepsi": "í©ì‹œ",
    "pep": "í©ì‹œ",
    "mcdonald": "ë§¥ë„ë‚ ë“œ",
    "mcdonalds": "ë§¥ë„ë‚ ë“œ",
    "mcd": "ë§¥ë„ë‚ ë“œ",
    "starbucks": "ìŠ¤íƒ€ë²…ìŠ¤",
    "sbux": "ìŠ¤íƒ€ë²…ìŠ¤",
    "nike": "ë‚˜ì´í‚¤",
    "nke": "ë‚˜ì´í‚¤",
    # ë¯¸ë””ì–´/ì—”í„°
    "netflix": "ë„·í”Œë¦­ìŠ¤",
    "nflx": "ë„·í”Œë¦­ìŠ¤",
    "disney": "ë””ì¦ˆë‹ˆ",
    "dis": "ë””ì¦ˆë‹ˆ",
    "sony": "ì†Œë‹ˆ",
    "sne": "ì†Œë‹ˆ",
    # ETFëŠ” ê·¸ëŒ€ë¡œ
}


def search_rag_data_for_news_persona(
    persona: str, news_title: str, news_content: str, companies: List[str] = None
) -> Dict[str, List[str]]:
    """
    ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ í˜ë¥´ì†Œë‚˜ë³„ RAG ê²€ìƒ‰ ìˆ˜í–‰

    Args:
        persona: í˜ë¥´ì†Œë‚˜ ì´ë¦„
        news_title: ë‰´ìŠ¤ ì œëª©
        news_content: ë‰´ìŠ¤ ë³¸ë¬¸
        companies: ê´€ë ¨ ê¸°ì—… ëª©ë¡

    Returns:
        {
            "postgres": [ê²°ê³¼1, ê²°ê³¼2, ...],
            "vector": [ê²°ê³¼1, ê²°ê³¼2, ...],
            "bm25": [ê²°ê³¼1, ê²°ê³¼2, ...]
        }
    """
    # í˜ë¥´ì†Œë‚˜ë³„ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    agent_db_config = AGENT_DATABASES.get(persona, {})

    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ë‰´ìŠ¤ ë‚´ìš© + ê´€ë ¨ ê¸°ì—…)
    search_query_parts = [news_title]
    if companies:
        search_query_parts.extend(companies[:3])  # ìµœëŒ€ 3ê°œ ê¸°ì—…ë§Œ ì‚¬ìš©

    search_query = " ".join(search_query_parts)

    # í˜ë¥´ì†Œë‚˜ë³„ í‚¤ì›Œë“œ ì¶”ê°€
    news_keywords = agent_db_config.get("news_keywords", [])
    if news_keywords:
        keyword_query = " ".join(news_keywords[:2])  # ìƒìœ„ 2ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        search_query = f"{search_query} {keyword_query}"

    # ê²€ìƒ‰ ê²°ê³¼
    results = {
        "postgres": [],
        "vector": [],
        "bm25": [],
    }

    # ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ (í˜ë¥´ì†Œë‚˜ë³„)
    chroma_collection_names = agent_db_config.get("chroma_collections", ["news_bge_m3"])
    chroma_collections = load_agent_collections(chroma_collection_names)

    # PostgreSQL ê²€ìƒ‰ (í˜ë¥´ì†Œë‚˜ë³„ í…Œì´ë¸” ì‚¬ìš©)
    if agent_db_config.get("use_postgres", False) and companies:
        postgres_tables = agent_db_config.get("postgres_tables", [])
        if postgres_tables:
            try:
                # ê´€ë ¨ ê¸°ì—…ë³„ë¡œ ê²€ìƒ‰
                for company in companies[:2]:  # ìµœëŒ€ 2ê°œ ê¸°ì—…ë§Œ ê²€ìƒ‰
                    pg_results, _ = search_postgres(
                        f"{company} {search_query}",
                        top_k=2,
                        postgres_tables=postgres_tables,
                    )
                    results["postgres"].extend(pg_results[:1])  # ê° ê¸°ì—…ë‹¹ 1ê°œì”©ë§Œ
            except Exception as e:
                print(f"âš ï¸ [{persona}] PostgreSQL ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    # ChromaDB ê²€ìƒ‰ (í˜ë¥´ì†Œë‚˜ë³„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼)
    if chroma_collections:
        search_priority = agent_db_config.get("search_priority", ["vector", "bm25"])

        # ë²¡í„° ê²€ìƒ‰
        if "vector" in search_priority:
            try:
                vector_results, _ = semantic_search_vector(
                    chroma_collections, search_query, top_k=2
                )
                results["vector"] = vector_results
            except Exception as e:
                print(f"âš ï¸ [{persona}] ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
        if "bm25" in search_priority:
            try:
                bm25_results, _ = keyword_search_bm25(
                    chroma_collections, search_query, top_k=2
                )
                results["bm25"] = bm25_results
            except Exception as e:
                print(f"âš ï¸ [{persona}] BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    return results


def analyze_news(title: str, content: str) -> Dict:
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 5ëª…ì˜ í˜ë¥´ì†Œë‚˜ ê´€ì ì—ì„œ ë¶„ì„ (RAG ê²€ìƒ‰ í™œìš©)
    - ë‰´ìŠ¤ ìš”ì•½
    - í˜ë¥´ì†Œë‚˜ 5ëª… ë¶„ì„ (í¬ì—´, ë•ìˆ˜, ì§€ìœ¨, í…Œì˜¤, ë¯¼ì§€)
    - ì˜í–¥ ë¯¸ì¹  ê¸°ì—… ëª©ë¡ ì¶”ì¶œ í›„ RAG ê²€ìƒ‰ ìˆ˜í–‰

    Args:
        title: ë‰´ìŠ¤ ì œëª©
        content: ë‰´ìŠ¤ ë³¸ë¬¸

    Returns:
        {
            "summary": str,
            "persona_analyses": {persona: analysis},
            "companies": [str]
        }
    """
    OPENAI_API_KEY = os.getenv("GMS_API_KEY")
    GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"

    if not OPENAI_API_KEY:
        raise ValueError("GMS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=GMS_BASE_URL)

    # ìƒì„¸ í˜ë¥´ì†Œë‚˜ ì„¤ëª…
    detailed_personas = "\n\n".join(
        [f"### {name}\n{desc}" for name, desc in AGENT_DESCRIPTIONS.items()]
    )

    # ì¶”ì  ëŒ€ìƒ ê¸°ì—… ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    tracked_companies_str = ", ".join(TRACKED_COMPANIES)

    # 1ë‹¨ê³„: ë¨¼ì € ê´€ë ¨ ê¸°ì—… ì¶”ì¶œ (RAG ì—†ì´ ë¹ ë¥´ê²Œ)
    initial_prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì§ì ‘ ì–¸ê¸‰ë˜ê±°ë‚˜ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ì—…/ETFë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

## ë‰´ìŠ¤ ê¸°ì‚¬
ì œëª©: {title}
ë‚´ìš©: {content[:1000]}

## ì¶”ì  ëŒ€ìƒ ê¸°ì—…/ETF ëª©ë¡
{tracked_companies_str}

## ìš”ì²­
ìœ„ ëª©ë¡ ì¤‘ì—ì„œ ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ë˜ê±°ë‚˜ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ì—…/ETFë§Œ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 5ê°œ).
ë‰´ìŠ¤ì— ì§ì ‘ ì–¸ê¸‰ëœ ê¸°ì—…ì´ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”.

## ì‘ë‹µ í˜•ì‹
{{"companies": ["ì• í”Œ", "í…ŒìŠ¬ë¼"]}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

    initial_response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that extracts company names from news. Always respond in valid JSON format only.",
            },
            {"role": "user", "content": initial_prompt},
        ],
        max_tokens=200,
        temperature=0.3,
        response_format={"type": "json_object"},
    )

    initial_result = json.loads(initial_response.choices[0].message.content.strip())
    extracted_companies = initial_result.get("companies", [])

    # ê¸°ì—…ëª… ë§¤ì¹­ (ì •í™•ë„ í–¥ìƒ)
    matched_companies = []
    tracked_lower = {c.lower(): c for c in TRACKED_COMPANIES}

    for company in extracted_companies:
        company_clean = company.strip()
        company_lower = company_clean.lower()

        if company_lower in tracked_lower:
            matched = tracked_lower[company_lower]
            if matched not in matched_companies:
                matched_companies.append(matched)
        elif company_lower in COMPANY_NAME_MAPPING:
            matched = COMPANY_NAME_MAPPING[company_lower]
            if matched not in matched_companies:
                matched_companies.append(matched)

    companies = matched_companies[:5]

    # 2ë‹¨ê³„: ê° í˜ë¥´ì†Œë‚˜ë³„ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰
    print(f"ğŸ” ë‰´ìŠ¤ ë¶„ì„ RAG ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘: {title[:50]}...")
    print(f"ğŸ“Œ ì¶”ì¶œëœ ê´€ë ¨ ê¸°ì—…: {companies}")
    all_rag_results = {}
    personas = ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]

    for persona in personas:
        all_rag_results[persona] = search_rag_data_for_news_persona(
            persona, title, content, companies
        )
        # RAG ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
        rag_results = all_rag_results[persona]
        print(f"\n[{persona}] RAG ê²€ìƒ‰ ê²°ê³¼:")
        if rag_results.get("postgres"):
            print(f"  PostgreSQL: {len(rag_results['postgres'])}ê°œ")
            for i, pg in enumerate(rag_results["postgres"][:2], 1):
                print(f"    {i}. {pg[:100]}...")
        if rag_results.get("vector"):
            print(f"  ë²¡í„° ê²€ìƒ‰: {len(rag_results['vector'])}ê°œ")
            for i, vec in enumerate(rag_results["vector"][:2], 1):
                print(f"    {i}. {vec[:100]}...")
        if rag_results.get("bm25"):
            print(f"  BM25 ê²€ìƒ‰: {len(rag_results['bm25'])}ê°œ")
            for i, bm in enumerate(rag_results["bm25"][:2], 1):
                print(f"    {i}. {bm[:100]}...")
        if not any(
            [
                rag_results.get("postgres"),
                rag_results.get("vector"),
                rag_results.get("bm25"),
            ]
        ):
            print(f"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

    # ê° í˜ë¥´ì†Œë‚˜ë³„ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
    persona_rag_contexts = {}
    for persona in personas:
        rag_results = all_rag_results.get(persona, {})
        rag_context = []

        # í˜ë¥´ì†Œë‚˜ë³„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê²°ê³¼ êµ¬ì„±
        agent_db_config = AGENT_DATABASES.get(persona, {})
        search_priority = agent_db_config.get(
            "search_priority", ["vector", "bm25", "postgres"]
        )

        for search_type in search_priority:
            if search_type == "postgres" and rag_results.get("postgres"):
                rag_context.append(f"[{persona} - ì¬ë¬´/ì£¼ê°€ ë°ì´í„°]")
                rag_context.extend(rag_results["postgres"][:2])
            elif search_type == "vector" and rag_results.get("vector"):
                rag_context.append(f"[{persona} - ê´€ë ¨ ë‰´ìŠ¤ - ì˜ë¯¸ ê²€ìƒ‰]")
                rag_context.extend(rag_results["vector"][:2])
            elif search_type == "bm25" and rag_results.get("bm25"):
                rag_context.append(f"[{persona} - ê´€ë ¨ ë‰´ìŠ¤ - í‚¤ì›Œë“œ ê²€ìƒ‰]")
                rag_context.extend(rag_results["bm25"][:2])

        persona_rag_contexts[persona] = (
            "\n".join(rag_context) if rag_context else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        )

    # RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    rag_sections = []
    for persona in personas:
        rag_section = f"### {persona}ì˜ ê²€ìƒ‰ ë°ì´í„°\n{persona_rag_contexts[persona]}"
        rag_sections.append(rag_section)

    rag_text_all = "\n\n".join(rag_sections)

    # 3ë‹¨ê³„: RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ ìµœì¢… ë¶„ì„ ìˆ˜í–‰
    prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì½ê³ , 5ëª…ì˜ íˆ¬ìì í˜ë¥´ì†Œë‚˜ê°€ ë‘ê´„ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë‰´ìŠ¤ ê¸°ì‚¬
ì œëª©: {title}

ë‚´ìš©:
{content[:2000]}

## í˜ë¥´ì†Œë‚˜ë³„ ê²€ìƒ‰ëœ ë°ì´í„° (ê° í˜ë¥´ì†Œë‚˜ëŠ” ìì‹ ì˜ ë°ì´í„°ë§Œ ì°¸ê³ )
{rag_text_all}

## í˜ë¥´ì†Œë‚˜ ì„¤ëª…
{detailed_personas}

## ì‘ì„± ìš”ì²­
ğŸ¯ ìµœìš°ì„  ëª©í‘œ: ì¬ë¯¸ìˆê²Œ ë§í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

1. **summary**: ë‰´ìŠ¤ì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

2. **persona_analyses**: ê° í˜ë¥´ì†Œë‚˜ê°€ ì´ ë‰´ìŠ¤ë¥¼ ì½ê³  ë‘ê´„ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
   - ë‘ê´„ì‹ìœ¼ë¡œ ì‘ì„±: í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
   - ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ íˆ¬ì ì² í•™ì— ë§ì¶° ê°•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”:
     * í¬ì—´: ê·¹ë„ë¡œ ì—´ì •ì ì´ê³  ê³µê²©ì ì¸ ë§íˆ¬. ì§§ê³  ê°•ë ¬í•œ ë¬¸ì¥, ê°íƒ„ì‚¬ì™€ ì´ëª¨í‹°ì½˜ ëŠë‚Œì˜ í‘œí˜„, ìˆ«ìì™€ ìˆ˜ìµë¥ ì„ ìì£¼ ì–¸ê¸‰. ê¸´ì¥ê°ê³¼ ì†ë„ê°, ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ì•Šìœ¼ë ¤ëŠ” ì ˆë°•í•¨ í‘œí˜„
     * ë•ìˆ˜: ì§€í˜œë¡­ê³  ì‹ ì¤‘í•œ ë§íˆ¬. ê¸´ ë¬¸ì¥, ë¹„ìœ ì™€ ì—­ì‚¬ì  ì‚¬ë¡€ë¥¼ ìì£¼ ì‚¬ìš©. ì‹ ì¤‘í•˜ê³  ì°¨ë¶„í•œ í†¤ìœ¼ë¡œ ìœ„í—˜ì„ ê²½ê³ . ë•Œë¡œëŠ” ë‚ ì¹´ë¡œìš´ ë¹„ê¼¼ìœ¼ë¡œ ë‹¨ê¸° íˆ¬ìë¥¼ ë¹„íŒ
     * ì§€ìœ¨: ëƒ‰í˜¹í•˜ê³  ê°ê´€ì ì¸ ë§íˆ¬. ì§§ê³  ëª…í™•í•œ ë¬¸ì¥, ìˆ«ìì™€ ì§€í‘œë¥¼ ìì£¼ ì–¸ê¸‰. ê°ì •ì„ ë°°ì œí•œ ëƒ‰í˜¹í•œ í†¤. ë•Œë¡œëŠ” ë‚ ì¹´ë¡œìš´ ë¹„ê¼¼ìœ¼ë¡œ ê°ì •ì  íˆ¬ì ë¹„íŒ
     * í…Œì˜¤: ë‚™ê´€ì ì´ê³  ë¯¸ë˜ì§€í–¥ì ì¸ ë§íˆ¬. ì¤‘ê°„ ê¸¸ì´ì˜ ë¬¸ì¥, ë¯¸ë˜ì§€í–¥ì  í‘œí˜„ê³¼ ì„±ì¥ë¥ ì„ ìì£¼ ì–¸ê¸‰. ê¸°ìˆ  í˜ì‹ ì˜ í¥ë¯¸ì§„ì§„í•¨ì„ í‘œí˜„. ì¥ê¸° ë¹„ì „ê³¼ ê³¨ë“ íƒ€ì„ì„ ê°•ì¡°
     * ë¯¼ì§€: ë¹ ë¥´ê³  ì§ê´€ì ì¸ ë§íˆ¬. ì§§ê³  ë¹ ë¥¸ ë¬¸ì¥, íŠ¸ë Œë“œ ìš©ì–´ì™€ ì†Œì…œ í‘œí˜„ì„ ìì£¼ ì‚¬ìš©. ë°ˆê³¼ í™”ì œì„±ì„ ì–¸ê¸‰. ì†Œì…œ ë°˜ì‘ê³¼ ì»¤ë®¤ë‹ˆí‹° ë¶„ìœ„ê¸°ë¥¼ ì½ëŠ” ê°ê°ì  í‘œí˜„
   - ë‰´ìŠ¤ì— ë‚˜ì˜¨ êµ¬ì²´ì ì¸ ë‚´ìš©(ê¸°ì—…ëª…, ìˆ˜ì¹˜, ì‚¬ê±´)ê³¼ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.
   - ë§¤ë²ˆ ë‹¤ë¥¸ ê´€ì ê³¼ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”. ê°™ì€ íŒ¨í„´ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
   - ì´ë¦„ ì–¸ê¸‰ ê¸ˆì§€! ë‹¤ë¥¸ ì‚¬ëŒì´ë‚˜ ìì‹ ì˜ ì´ë¦„ì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

3. **companies**: {companies if companies else "[]"} (ì´ë¯¸ ì¶”ì¶œë¨)

## ì‘ë‹µ í˜•ì‹
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "summary": "ë‰´ìŠ¤ ìš”ì•½ (2-3ë¬¸ì¥)",
  "persona_analyses": {{
    "í¬ì—´": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "ë•ìˆ˜": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "ì§€ìœ¨": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "í…Œì˜¤": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "ë¯¼ì§€": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
  }},
  "companies": {companies if companies else []}
}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that analyzes news and provides investment opinions from multiple personas. Always respond in valid JSON format only.",
            },
            {"role": "user", "content": prompt},
        ],
        max_tokens=2000,  # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
        temperature=0.9,  # ë‹¤ì–‘ì„± í–¥ìƒì„ ìœ„í•´ temperature ì¦ê°€
        response_format={"type": "json_object"},  # JSON í˜•ì‹ ê°•ì œ
    )

    result_text = response.choices[0].message.content.strip()

    # JSON íŒŒì‹±
    try:
        result_json = json.loads(result_text)
    except json.JSONDecodeError:
        raise ValueError("ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")

    # ê²°ê³¼ ê²€ì¦ ë° ë°˜í™˜
    summary = result_json.get("summary", "ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
    persona_analyses = result_json.get("persona_analyses", {})
    # companiesëŠ” ì´ë¯¸ ì¶”ì¶œë˜ì–´ ìˆìŒ

    # í˜ë¥´ì†Œë‚˜ ë¶„ì„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    for persona in ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]:
        if persona not in persona_analyses:
            persona_analyses[persona] = f"{persona} ë¶„ì„ ìƒì„± ì‹¤íŒ¨"

    return {
        "summary": summary,
        "persona_analyses": persona_analyses,
        "companies": companies,
    }
