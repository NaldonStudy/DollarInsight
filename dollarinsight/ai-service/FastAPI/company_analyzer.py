"""
ê¸°ì—… ë¶„ì„ ëª¨ë“ˆ
- ê¸°ì—… í‚¤ì›Œë“œë¥¼ ë°›ì•„ì„œ í˜ë¥´ì†Œë‚˜ë³„ë¡œ í•œë§ˆë””ì”© íˆ¬ì ì˜ê²¬ ìƒì„±
- RAG ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë¶„ì„
"""

import os
import json
from typing import Dict, List
from datetime import datetime
import openai

from prompts import AGENT_DESCRIPTIONS, AGENT_DATABASES
from database import (
    load_agent_collections,
    search_postgres,
    get_table_schema_info,
    get_schema_cache,
)
from search import keyword_search_bm25, semantic_search_vector


def search_rag_data_for_persona(
    persona: str, company_name: str, company_info: str = ""
) -> Dict[str, List[str]]:
    """
    íŠ¹ì • í˜ë¥´ì†Œë‚˜ì— ë§ëŠ” RAG ê²€ìƒ‰ ìˆ˜í–‰

    Args:
        persona: í˜ë¥´ì†Œë‚˜ ì´ë¦„ ("í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€")
        company_name: ê¸°ì—…ëª…
        company_info: ê¸°ì—… ì •ë³´

    Returns:
        {
            "postgres": [ê²°ê³¼1, ê²°ê³¼2, ...],
            "vector": [ê²°ê³¼1, ê²°ê³¼2, ...],
            "bm25": [ê²°ê³¼1, ê²°ê³¼2, ...]
        }
    """
    # í˜ë¥´ì†Œë‚˜ë³„ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    agent_db_config = AGENT_DATABASES.get(persona, {})

    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
    search_query = f"{company_name} {company_info}".strip()

    # í˜ë¥´ì†Œë‚˜ë³„ í‚¤ì›Œë“œ ì¶”ê°€ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
    news_keywords = agent_db_config.get("news_keywords", [])
    if news_keywords:
        keyword_query = " ".join(news_keywords[:3])  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        search_query = f"{search_query} {keyword_query}"

    # ê²€ìƒ‰ ê²°ê³¼
    results = {
        "postgres": [],
        "vector": [],
        "bm25": [],
    }

    # ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ (í˜ë¥´ì†Œë‚˜ë³„)
    chroma_collection_names = agent_db_config.get("chroma_collections", ["news_bge_m3"])
    chroma_collections = load_agent_collections(chroma_collection_names)

    # PostgreSQL ê²€ìƒ‰ (í˜ë¥´ì†Œë‚˜ë³„ í…Œì´ë¸” ì‚¬ìš©)
    if agent_db_config.get("use_postgres", False):
        postgres_tables = agent_db_config.get("postgres_tables", [])
        if postgres_tables:
            try:
                pg_results, _ = search_postgres(
                    search_query, top_k=3, postgres_tables=postgres_tables
                )
                results["postgres"] = pg_results
            except Exception as e:
                print(f"âš ï¸ [{persona}] PostgreSQL ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    # ChromaDB ê²€ìƒ‰ (í˜ë¥´ì†Œë‚˜ë³„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼)
    if chroma_collections:
        search_priority = agent_db_config.get("search_priority", ["vector", "bm25"])

        # ë²¡í„° ê²€ìƒ‰
        if "vector" in search_priority:
            try:
                vector_results, _ = semantic_search_vector(
                    chroma_collections, search_query, top_k=3
                )
                results["vector"] = vector_results
            except Exception as e:
                print(f"âš ï¸ [{persona}] ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        # BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
        if "bm25" in search_priority:
            try:
                bm25_results, _ = keyword_search_bm25(
                    chroma_collections, search_query, top_k=3
                )
                results["bm25"] = bm25_results
            except Exception as e:
                print(f"âš ï¸ [{persona}] BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    return results


def search_rag_data_all_personas(
    company_name: str, company_info: str = ""
) -> Dict[str, Dict[str, List[str]]]:
    """
    ëª¨ë“  í˜ë¥´ì†Œë‚˜ë³„ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰

    Returns:
        {
            "í¬ì—´": {"postgres": [...], "vector": [...], "bm25": [...]},
            "ë•ìˆ˜": {"postgres": [...], "vector": [...], "bm25": [...]},
            ...
        }
    """
    personas = ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]
    all_results = {}

    for persona in personas:
        all_results[persona] = search_rag_data_for_persona(
            persona, company_name, company_info
        )

    return all_results


def analyze_company(company_name: str, company_info: str = "") -> Dict:
    """
    ê¸°ì—…ì„ 5ëª…ì˜ í˜ë¥´ì†Œë‚˜ ê´€ì ì—ì„œ ë¶„ì„ (RAG ê²€ìƒ‰ í™œìš©)
    - RAG ê²€ìƒ‰ì„ í†µí•´ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë¶„ì„
    - í˜ë¥´ì†Œë‚˜ë³„ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ íˆ¬ì ì˜ê²¬ ìƒì„±

    Args:
        company_name: ê¸°ì—…ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì", "AAPL", "í…ŒìŠ¬ë¼")
        company_info: ê¸°ì—… ì •ë³´ (ì„ íƒì‚¬í•­, ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸)

    Returns:
        {
            "company_name": str,
            "persona_analyses": {persona: analysis},
            "analyzed_at": str
        }
    """
    OPENAI_API_KEY = os.getenv("GMS_API_KEY")
    GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"

    if not OPENAI_API_KEY:
        raise ValueError("GMS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=GMS_BASE_URL)

    # ê° í˜ë¥´ì†Œë‚˜ë³„ë¡œ RAG ê²€ìƒ‰ ìˆ˜í–‰
    print(f"ğŸ” í˜ë¥´ì†Œë‚˜ë³„ RAG ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘: {company_name}")
    all_rag_results = search_rag_data_all_personas(company_name, company_info)

    # RAG ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    personas = ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]
    for persona in personas:
        rag_results = all_rag_results.get(persona, {})
        print(f"\n[{persona}] RAG ê²€ìƒ‰ ê²°ê³¼:")
        if rag_results.get("postgres"):
            print(f"  PostgreSQL: {len(rag_results['postgres'])}ê°œ")
            for i, pg in enumerate(rag_results["postgres"][:2], 1):
                print(f"    {i}. {pg[:100]}...")
        if rag_results.get("vector"):
            print(f"  ë²¡í„° ê²€ìƒ‰: {len(rag_results['vector'])}ê°œ")
            for i, vec in enumerate(rag_results["vector"][:2], 1):
                print(f"    {i}. {vec[:100]}...")
        if rag_results.get("bm25"):
            print(f"  BM25 ê²€ìƒ‰: {len(rag_results['bm25'])}ê°œ")
            for i, bm in enumerate(rag_results["bm25"][:2], 1):
                print(f"    {i}. {bm[:100]}...")
        if not any(
            [
                rag_results.get("postgres"),
                rag_results.get("vector"),
                rag_results.get("bm25"),
            ]
        ):
            print(f"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

    # ê° í˜ë¥´ì†Œë‚˜ë³„ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
    persona_rag_contexts = {}
    personas = ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]

    for persona in personas:
        rag_results = all_rag_results.get(persona, {})
        rag_context = []

        # í˜ë¥´ì†Œë‚˜ë³„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê²°ê³¼ êµ¬ì„±
        agent_db_config = AGENT_DATABASES.get(persona, {})
        search_priority = agent_db_config.get(
            "search_priority", ["vector", "bm25", "postgres"]
        )

        for search_type in search_priority:
            if search_type == "postgres" and rag_results.get("postgres"):
                rag_context.append(f"[{persona} - ì¬ë¬´/ì£¼ê°€ ë°ì´í„°]")
                rag_context.extend(rag_results["postgres"][:2])
            elif search_type == "vector" and rag_results.get("vector"):
                rag_context.append(f"[{persona} - ê´€ë ¨ ë‰´ìŠ¤ - ì˜ë¯¸ ê²€ìƒ‰]")
                rag_context.extend(rag_results["vector"][:2])
            elif search_type == "bm25" and rag_results.get("bm25"):
                rag_context.append(f"[{persona} - ê´€ë ¨ ë‰´ìŠ¤ - í‚¤ì›Œë“œ ê²€ìƒ‰]")
                rag_context.extend(rag_results["bm25"][:2])

        persona_rag_contexts[persona] = (
            "\n".join(rag_context) if rag_context else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        )

    # ìƒì„¸ í˜ë¥´ì†Œë‚˜ ì„¤ëª…
    detailed_personas = "\n\n".join(
        [f"### {name}\n{desc}" for name, desc in AGENT_DESCRIPTIONS.items()]
    )

    # ê¸°ì—… ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨
    company_context = f"\n\nê¸°ì—… ì •ë³´:\n{company_info}" if company_info else ""

    # ê° í˜ë¥´ì†Œë‚˜ë³„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•œ ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    rag_sections = []
    for persona in personas:
        rag_section = f"### {persona}ì˜ ê²€ìƒ‰ ë°ì´í„°\n{persona_rag_contexts[persona]}"
        rag_sections.append(rag_section)

    rag_text_all = "\n\n".join(rag_sections)

    # ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¤ìŒ ê¸°ì—…ì— ëŒ€í•´ 5ëª…ì˜ íˆ¬ìì í˜ë¥´ì†Œë‚˜ê°€ ë‘ê´„ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ íˆ¬ì ì˜ê²¬ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ë¶„ì„ ëŒ€ìƒ ê¸°ì—…
{company_name}
{company_context}

## í˜ë¥´ì†Œë‚˜ë³„ ê²€ìƒ‰ëœ ë°ì´í„° (ê° í˜ë¥´ì†Œë‚˜ëŠ” ìì‹ ì˜ ë°ì´í„°ë§Œ ì°¸ê³ )
{rag_text_all}

## í˜ë¥´ì†Œë‚˜ ì„¤ëª…
{detailed_personas}

## ì‘ì„± ê°€ì´ë“œ
ğŸ¯ ìµœìš°ì„  ëª©í‘œ: ì¬ë¯¸ìˆê²Œ ë§í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

âš ï¸ í•„ìˆ˜ ì‚¬í•­:
- ë‘ê´„ì‹ìœ¼ë¡œ ì‘ì„±: í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
- ì´ ê¸°ì—…ì˜ ê³ ìœ í•œ íŠ¹ì„±(ì—…ì¢…, ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸, ìµœê·¼ ë‰´ìŠ¤, ì¬ë¬´ ìƒíƒœ)ì— ì§‘ì¤‘í•˜ì„¸ìš”.
- ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ íˆ¬ì ì² í•™ì— ë§ì¶° ê°•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”:
  * í¬ì—´: ê·¹ë„ë¡œ ì—´ì •ì ì´ê³  ê³µê²©ì ì¸ ë§íˆ¬. ì§§ê³  ê°•ë ¬í•œ ë¬¸ì¥, ê°íƒ„ì‚¬ì™€ ì´ëª¨í‹°ì½˜ ëŠë‚Œì˜ í‘œí˜„, ìˆ«ìì™€ ìˆ˜ìµë¥ ì„ ìì£¼ ì–¸ê¸‰. ê¸´ì¥ê°ê³¼ ì†ë„ê°, ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ì•Šìœ¼ë ¤ëŠ” ì ˆë°•í•¨ í‘œí˜„
  * ë•ìˆ˜: ì§€í˜œë¡­ê³  ì‹ ì¤‘í•œ ë§íˆ¬. ê¸´ ë¬¸ì¥, ë¹„ìœ ì™€ ì—­ì‚¬ì  ì‚¬ë¡€ë¥¼ ìì£¼ ì‚¬ìš©. ì‹ ì¤‘í•˜ê³  ì°¨ë¶„í•œ í†¤ìœ¼ë¡œ ìœ„í—˜ì„ ê²½ê³ . ë•Œë¡œëŠ” ë‚ ì¹´ë¡œìš´ ë¹„ê¼¼ìœ¼ë¡œ ë‹¨ê¸° íˆ¬ìë¥¼ ë¹„íŒ
  * ì§€ìœ¨: ëƒ‰í˜¹í•˜ê³  ê°ê´€ì ì¸ ë§íˆ¬. ì§§ê³  ëª…í™•í•œ ë¬¸ì¥, ìˆ«ìì™€ ì§€í‘œë¥¼ ìì£¼ ì–¸ê¸‰. ê°ì •ì„ ë°°ì œí•œ ëƒ‰í˜¹í•œ í†¤. ë•Œë¡œëŠ” ë‚ ì¹´ë¡œìš´ ë¹„ê¼¼ìœ¼ë¡œ ê°ì •ì  íˆ¬ì ë¹„íŒ
  * í…Œì˜¤: ë‚™ê´€ì ì´ê³  ë¯¸ë˜ì§€í–¥ì ì¸ ë§íˆ¬. ì¤‘ê°„ ê¸¸ì´ì˜ ë¬¸ì¥, ë¯¸ë˜ì§€í–¥ì  í‘œí˜„ê³¼ ì„±ì¥ë¥ ì„ ìì£¼ ì–¸ê¸‰. ê¸°ìˆ  í˜ì‹ ì˜ í¥ë¯¸ì§„ì§„í•¨ì„ í‘œí˜„. ì¥ê¸° ë¹„ì „ê³¼ ê³¨ë“ íƒ€ì„ì„ ê°•ì¡°
  * ë¯¼ì§€: ë¹ ë¥´ê³  ì§ê´€ì ì¸ ë§íˆ¬. ì§§ê³  ë¹ ë¥¸ ë¬¸ì¥, íŠ¸ë Œë“œ ìš©ì–´ì™€ ì†Œì…œ í‘œí˜„ì„ ìì£¼ ì‚¬ìš©. ë°ˆê³¼ í™”ì œì„±ì„ ì–¸ê¸‰. ì†Œì…œ ë°˜ì‘ê³¼ ì»¤ë®¤ë‹ˆí‹° ë¶„ìœ„ê¸°ë¥¼ ì½ëŠ” ê°ê°ì  í‘œí˜„
- ê²€ìƒ‰ëœ ë°ì´í„°ì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë‰´ìŠ¤ë¥¼ í™œìš©í•˜ì„¸ìš”.
- ë§¤ë²ˆ ë‹¤ë¥¸ ê´€ì ê³¼ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”. ê°™ì€ íŒ¨í„´ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
- ì´ë¦„ ì–¸ê¸‰ ê¸ˆì§€! ë‹¤ë¥¸ ì‚¬ëŒì´ë‚˜ ìì‹ ì˜ ì´ë¦„ì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

## ì‘ë‹µ í˜•ì‹
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "persona_analyses": {{
    "í¬ì—´": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "ë•ìˆ˜": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "ì§€ìœ¨": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "í…Œì˜¤": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "ë¯¼ì§€": "í•µì‹¬ ì˜ê²¬ì„ ë¨¼ì € ì œì‹œí•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê° í˜ë¥´ì†Œë‚˜ì˜ ê³ ìœ í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
  }}
}}

âš ï¸ ì¤‘ìš”: ê° ê¸°ì—…ë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©ê³¼ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”. ê°™ì€ íŒ¨í„´ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that analyzes companies from multiple investment personas. Always respond in valid JSON format only. Make the responses natural and conversational, not formulaic. CRITICAL: Each company must be analyzed with completely different perspectives and expressions. Focus on each company's unique characteristics, industry, business model, and recent news. Never use similar patterns across different companies.",
            },
            {"role": "user", "content": prompt},
        ],
        max_tokens=2000,  # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
        temperature=1.0,  # ë‹¤ì–‘ì„± ìµœëŒ€í™”ë¥¼ ìœ„í•´ temperature ì¦ê°€ (0.9 -> 1.0)
        response_format={"type": "json_object"},  # JSON í˜•ì‹ ê°•ì œ
    )

    result_text = response.choices[0].message.content.strip()

    # JSON íŒŒì‹±
    try:
        result_json = json.loads(result_text)
    except json.JSONDecodeError:
        raise ValueError("ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")

    # ê²°ê³¼ ê²€ì¦ ë° ë°˜í™˜
    persona_analyses = result_json.get("persona_analyses", {})

    # í˜ë¥´ì†Œë‚˜ ë¶„ì„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    for persona in ["í¬ì—´", "ë•ìˆ˜", "ì§€ìœ¨", "í…Œì˜¤", "ë¯¼ì§€"]:
        if persona not in persona_analyses:
            persona_analyses[persona] = f"{persona} ë¶„ì„ ìƒì„± ì‹¤íŒ¨"

    # í˜ë¥´ì†Œë‚˜ë³„ë¡œ ê°œë³„ í•„ë“œë¡œ ë³€í™˜ (ì˜ë¬¸ ì»¬ëŸ¼ëª… ì‚¬ìš©)
    result = {
        "company_name": company_name,
        "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    # ê° í˜ë¥´ì†Œë‚˜ë¥¼ ì˜ë¬¸ í•„ë“œëª…ìœ¼ë¡œ ì¶”ê°€
    persona_mapping = {
        "í¬ì—´": "heuyeol",
        "ë•ìˆ˜": "deoksu",
        "ì§€ìœ¨": "jiyul",
        "í…Œì˜¤": "teo",
        "ë¯¼ì§€": "minji",
    }

    for korean_name, english_name in persona_mapping.items():
        result[english_name] = persona_analyses.get(
            korean_name, f"{korean_name} ë¶„ì„ ìƒì„± ì‹¤íŒ¨"
        )

    return result
